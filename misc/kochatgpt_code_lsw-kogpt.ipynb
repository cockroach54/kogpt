{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLUYAx-jSMVQ"
   },
   "source": [
    "# Make ChatGPT-replica!!\n",
    "\n",
    "230421, by wygo\n",
    "\n",
    "#### 230421\n",
    "- GPU ì„¤ì • í›„ ëª¨ë‘ì‹¤í–‰ í•˜ë©´ STEP1~3ê¹Œì§€ ëª¨ë‘ ëŒì•„ê°ˆìˆ˜ìˆê²Œ ìˆ˜ì •\n",
    "- torch 1.xëŒ€ ë²„ì „ìœ¼ë¡œ ë‹¤ìš´\n",
    "- RMëª¨ë¸ ì ìˆ˜í™•ì¸ì½”ë“œ ì¶”ê°€\n",
    "\n",
    "#### 230320\n",
    "- ChatGPTëŠ” ê³µê°œ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.\n",
    "- ë³¸ ì„¸ë¯¸ë‚˜ì—ì„œëŠ” ChatGPTë¥¼ ë§Œë“  ì›ë¦¬ì¸ GPT fine-tuning, ê°•í™”í•™ìŠµ(PPO), RLHF, ChatGPT ë°ì´í„°ì…‹ êµ¬ì¶•ì— ëŒ€í•´ ë‹¤ë£¨ê³  ì½”ë“œ ì‹¤ìŠµì„ í•©ë‹ˆë‹¤.\n",
    "- ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ í™œìš©ë§Œ í•˜ëŠ” ê±´ ì¬ë¯¸ì—†ì–ì•„ìš”??\n",
    "- ìš°ë¦¬ ë¶„ì•¼ë§Œì˜ ChatGPT(í•œêµ­ì–´/ì „ë¬¸ë¶„ì•¼)ë¥¼ ì§ì ‘ ë§Œë“œëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "    - â€» êµ¬í˜„ ëª¨ë¸ì€ ChatGPT-replicaì…ë‹ˆë‹¤. ì‹¤ì œ ChatGPTì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    - â€» GPT3ê°€ ì•„ë‹Œ GPT2+RLHFë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. ê±°ëŒ€ì–¸ì–´ëª¨ë¸ë¡œ ê°œë°œì‹œ ì–´ë ¤ì›€ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    - â€» ì‹¤ìŠµí™˜ê²½: Jupyter or Colab, ì„ ìˆ˜ ì§€ì‹: íŒŒì´ì¬\n",
    "\n",
    "\n",
    "### Reference\n",
    "- [code_TRL](https://github.com/lvwerra/trl)\n",
    "- [code_Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n",
    "- [code_Alpaca_lora](https://github.com/tloen/alpaca-lora)\n",
    "\n",
    "- [blog_colossal-ai-chatgpt](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt)\n",
    "- [blog_RLHF_huggingface](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "\n",
    "### ChatGPT í•™ìŠµ ë°©ë²•\n",
    "- STEP 1) Prompt ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ìƒ˜í”Œë§í•˜ê³  ì‚¬ëŒì˜ ì‘ë‹µì„ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ fine-tuning\n",
    "- STEP 2) Prompt ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ìƒ˜í”Œë§í•˜ê³ , ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì‘ë‹µì„ ìƒì„±, ì´ëŸ¬í•œ ì‘ë‹µì˜ ìˆœìœ„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•˜ê³ , ì¸ê°„ì˜ ì„ í˜¸ë„ì— ë§ê²Œ ë³´ìƒ ëª¨ë¸(Reward Model)ì„ í•™ìŠµ\n",
    "- STEP 3) 1ë‹¨ê³„ì˜ ì§€ë„ ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ê³¼ 2ë‹¨ê³„ì˜ ë³´ìƒ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì¶”ê°€ë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "###  ChatGPT ê°œë°œ Requirement\n",
    "- ë°ì´í„°(RLHF)\n",
    "- LLM ëª¨ë¸(GPT3ê¸‰)\n",
    "- GPU\n",
    "    - ìˆ˜ì²œ GBì˜ GPU ë©”ëª¨ë¦¬ê°€ í•„ìš”\n",
    "    - ì¼ë°˜ì ì¸ ë°ì´í„° ë³‘ë ¬ ê¸°ìˆ ë¡œë„ X\n",
    "    - ìµœì†Œ 64ê°œì˜ 80GB A100 GPUê°€ í•„ìš”\n",
    "\n",
    "###  ChatGPT-replica ì‹¤ìŠµ Requirement\n",
    "- ë°ì´í„°(RLHF): data_kochatgpt\n",
    "- LLM ëª¨ë¸: GPT2\n",
    "- GPU: Colab\n",
    "\n",
    "### ChatGPT-replica ëª¨ë¸ ì •ë¦¬\n",
    "- [Step1_fine tuning code](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
    "- [huggingface_TRL](https://github.com/lvwerra/trl)\n",
    "    - RLì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì½”ë“œ ì œê³µ\n",
    "    - ChatGPTë¥¼ ìœ„í•´ ì½”ë“œìˆ˜ì •ì´ ë§ì´ë§ì´ í•„ìš”í•¨\n",
    "- [LLaMA](https://github.com/facebookresearch/llama)\n",
    "    - ChatGPTë³´ë‹¤ ëª¨ë¸ í¬ê¸°ê°€ ì‘ìœ¼ë©´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ê³µê°œ\n",
    "    - í•œêµ­ì–´..ì¶”ê°€í•™ìŠµ..\n",
    "- [ChatLLaMA](https://github.com/juncongmoo/chatllama)\n",
    "    - LLaMAë¥¼ Chat í˜•ì‹ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ê°•í™”í•™ìŠµ ì½”ë“œ ì œê³µ\n",
    "    - GPT3ê¸°ë°˜ [ëŒ€í™” ë°ì´í„°ì…‹ êµ¬ì¶• ì½”ë“œ](https://github.com/juncongmoo/chatllama/blob/main/generate_dataset.py) ì œê³µ, ìˆ˜ì • ë§ì´ í•„ìš” \n",
    "- [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n",
    "    - Instruction ë°ì´í„° ìƒì„± ë° SFTë§Œ\n",
    "\n",
    "- [KoAlpaca](https://github.com/Beomi/KoAlpaca)\n",
    "    - í•œêµ­ì–´ Instruction ë°ì´í„° ìƒì„± ë° SFTë§Œ\n",
    "\n",
    "- [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)\n",
    "    - Low-Rank LLaMA Instruct-Tuning, SFTë§Œ\n",
    "\n",
    "    \n",
    "- **[ColossalAI](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ChatGPT)**\n",
    "    - step2 RM í•™ìŠµê³¼ step3 PPO ì½”ë“œ ê¹”ë”í•˜ê²Œ ì œê³µ\n",
    "    - Multi-GPUë¡œ DDP, ColossalAIStrategy, LoRA í•™ìŠµì½”ë“œ ì œê³µ!!\n",
    "    \n",
    "- **ColossalAI ì¥ì **\n",
    "    - ColossalAIëŠ” pytorchì— ë¹„í•´ ì¶”ë¡ ì‹œ 1.4ë°° ë¹ ë¥´ê³ , í•™ìŠµì‹œ 7.7ë°° ë¹ ë¥´ë‹¤!!\n",
    "    - ColossalAIëŠ” pytorchì™€ ë¹„êµí•´ 10.3ë°° í° ëª¨ë¸ì„ ì²˜ë¦¬í• ìˆ˜ ìˆë‹¤!!\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png\" width=\"800\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "-_Cqx0AySMVS"
   },
   "source": [
    "### Step 0) Prepare RLHF dataset\n",
    "\n",
    "- ì´ 3stepì„ í•™ìŠµí•˜ê¸° ìœ„í•´ 3ê°€ì§€ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- [ë°ì´í„°ì…‹ ì˜ˆì‹œ](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama#dataset-preparation)\n",
    "- [ì˜ˆì‹œ ë°ì´í„°ì…‹ 1](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
    "- [ì˜ˆì‹œ ë°ì´í„°ì…‹ 2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
    "\n",
    "step1) SFT(actor_training_data): SFT ì§€ë„ ë¯¸ì„¸ ì¡°ì •ì— ì‚¬ìš©ë˜ëŠ” JSON ë°ì´í„°\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\",\n",
    "        \"completion\": \"\"        \n",
    "    }, ...\n",
    "]\n",
    "```\n",
    "\n",
    "step2) RM ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹(reward_training_data): ë³´ìƒ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” JSON ë°ì´í„°ì…‹. í•œ promptì— ëŒ€í•´ ì—¬ëŸ¬ ì™„ì„±ëœ ë¬¸ì¥ì´ ìˆê³ , ì´ ë¬¸ì¥ë“¤ì˜ rankingì„ ì‚¬ëŒì´ ë§¤ê¹€\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\",\n",
    "        \"completion_1\": \"\",\n",
    "        \"completion_2\": \"\",\n",
    "        \"completion_3\": \"\",            \n",
    "        \"ranking\": [1, 0, 2]\n",
    "    }, ...\n",
    "]\n",
    "```\n",
    "    \n",
    "step3) PPO í•™ìŠµ ì…ë ¥ ë°ì´í„°ì…‹(rlhf_training_data): RLHF í›ˆë ¨ì— ì‚¬ìš©ë˜ëŠ” JSON ë°ì´í„°ì…‹, ì‚¬ìš©ì ì…ë ¥ promptë¡œë§Œ êµ¬ì„±\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\"\n",
    "    }, ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "7igAgV-5SMVT"
   },
   "source": [
    "### step0) Colab í™˜ê²½ ì„¤ì •\n",
    "\n",
    "#### ì„¤ì¹˜(python>=3.8)\n",
    "```python\n",
    "## setup(1min)\n",
    "# torch ë²„ì „ ë‹¤ìš´. torch>=2.0 ì—ì„  colosalaiê°€ ë™ì‘ì•ˆí•¨\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__))\n",
    "print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!mv KoChatGPT/data_kochatgpt .\n",
    "!mv KoChatGPT/img .\n",
    "\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.13.1\n",
      "cuda version: 11.6\n",
      "cudnn version:8302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__))\n",
    "print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "LIg6pniqSMVU",
    "outputId": "1c57d04f-a2e3-4747-b76f-27d35750a28c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc17108f9d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/colossalai/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc17108e9e0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/colossalai/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting colossalai==0.2.7\n",
      "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (1.22.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (5.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (23.0)\n",
      "Collecting pre-commit\n",
      "  Downloading pre_commit-3.3.1-py2.py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich\n",
      "  Downloading rich-13.3.5-py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m238.7/238.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (8.1.3)\n",
      "Collecting fabric\n",
      "  Downloading fabric-3.0.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contexttimer\n",
      "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (1.11.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (1.13.1)\n",
      "Collecting invoke>=2.0\n",
      "  Downloading invoke-2.1.1-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting paramiko>=2.4\n",
      "  Downloading paramiko-3.1.0-py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nodeenv>=0.11.1\n",
      "  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n",
      "Collecting identify>=1.0.0\n",
      "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cfgv>=2.0.0\n",
      "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from pre-commit->colossalai==0.2.7) (6.0)\n",
      "Collecting virtualenv>=20.10.0\n",
      "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from torch->colossalai==0.2.7) (4.4.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (65.5.0)\n",
      "Collecting bcrypt>=3.2\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m593.2/593.2 kB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynacl>=1.5\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (38.0.1)\n",
      "Collecting filelock<4,>=3.11\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs<4,>=3.2\n",
      "  Downloading platformdirs-3.5.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
      "Building wheels for collected packages: colossalai, contexttimer\n",
      "  Building wheel for colossalai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896481 sha256=63622119541117851978680a9d7f1eb27e76976fe7069003ab9c5198d3804bdb\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/0c/72/9b13e70297cc84e934883264ca7ca4763d1ce867328d73d491\n",
      "  Building wheel for contexttimer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5805 sha256=4d8dbe3e11a23c3f46570dbffebd2891a043060129a44845f2fb936681e6aba1\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/11/19/7331f9a8b7c18c5a2c00a6a5d83d5767c831bdc1c0c0bce247\n",
      "Successfully built colossalai contexttimer\n",
      "Installing collected packages: distlib, contexttimer, pygments, platformdirs, nodeenv, mdurl, invoke, identify, filelock, cfgv, bcrypt, virtualenv, pynacl, markdown-it-py, rich, pre-commit, paramiko, fabric, colossalai\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.1.1\n",
      "    Uninstalling platformdirs-3.1.1:\n",
      "      Successfully uninstalled platformdirs-3.1.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.6.0\n",
      "    Uninstalling filelock-3.6.0:\n",
      "      Successfully uninstalled filelock-3.6.0\n",
      "Successfully installed bcrypt-4.0.1 cfgv-3.3.1 colossalai-0.2.7 contexttimer-0.3.3 distlib-0.3.6 fabric-3.0.1 filelock-3.12.0 identify-2.5.24 invoke-2.1.1 markdown-it-py-2.2.0 mdurl-0.1.2 nodeenv-1.7.0 paramiko-3.1.0 platformdirs-3.5.0 pre-commit-3.3.1 pygments-2.15.1 pynacl-1.5.0 rich-13.3.5 virtualenv-20.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m[Errno 2] No such file or directory: 'KoChatGPT/colossalai_ChatGPT_230319/'\n",
      "/workspace/KoChatGPT\n",
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m/\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.10/site-packages (0.27.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain==0.0.113\n",
      "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (1.10.7)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from langchain==0.0.113) (6.0)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (1.22.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (2.28.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (3.8.4)\n",
      "Collecting SQLAlchemy<2,>=1\n",
      "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.4)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.113) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m613.7/613.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, mypy-extensions, marshmallow, greenlet, typing-inspect, SQLAlchemy, marshmallow-enum, dataclasses-json, langchain\n",
      "Successfully installed SQLAlchemy-1.4.48 dataclasses-json-0.5.7 greenlet-2.0.2 langchain-0.0.113 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 tenacity-8.2.2 typing-inspect-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ## setup(1min)\n",
    "# # torch ë²„ì „ ë‹¤ìš´. torch>=2.0 ì—ì„  colosalaiê°€ ë™ì‘ì•ˆí•¨\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"Torch version:{}\".format(torch.__version__))\n",
    "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd ./colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tyf1bUtSMVU"
   },
   "source": [
    "### Step 1) SFT: ì§ˆë¬¸ì— ëŒ€ë‹µì„ ì˜í•˜ëŠ” ëª¨ë¸ ë§Œë“¤ê¸°\n",
    "- [fine tuning code_1](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
    "- [fine tuning code_2](https://github.com/Beomi/KoAlpaca/blob/main/train.py)\n",
    "\n",
    "- SFT(Supervised Fine Tuning)\n",
    "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
    "- ê¸°ì¡´ GPT3ëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ ë§ì¶”ëŠ” ëª¨ë¸. But ì§ˆë¬¸ì— ëŒ€í•´ ë‹µì„ ë§ì¶”ëŠ” ëª¨ë¸ì´ X\n",
    "- ì§ˆë¬¸ì— ì‘ë‹µì„ ì˜í•˜ë„ë¡ SFT ìˆ˜í–‰\n",
    "- ë¨¼ì € ì‚¬ëŒì´ ì§€ì‹œì— ëŒ€í•œ ëŒ€ë‹µì„ ì§ì ‘ ì‘ì„±(ë°ì´í„° 13,000ê°œ)í•˜ê³ , ì´ ë°ì´í„°ì…‹ìœ¼ë¡œ SFT\n",
    "- ë°ì´í„°: ì§ˆë¬¸-ì‘ë‹µ ìŒ ë°ì´í„°ì…‹(12,000ê°œ)\n",
    "- ì˜ˆì‹œ)\n",
    "    - ì§ˆë¬¸(prompt): ì¸ê³µì§€ëŠ¥ì„ ì„¤ëª…í•´ë³´ì„¸ìš”\n",
    "    - ì‘ë‹µ(completion): ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•˜ë ¤ëŠ” ì»´í“¨í„° ê³¼í•™ì˜ ì„¸ë¶€ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì´ë‹¤. ...  \n",
    "\n",
    "\n",
    "- **SFT ì˜ˆì‹œ**  \n",
    "<img src=\"img/1_SFT_1.png\" width=\"500\">  \n",
    "\n",
    "- **ëª¨ë¸ ì…ì¶œë ¥ ì˜ˆì‹œ**  \n",
    "<img src=\"img/image_step1.JPG\" width=\"500\">  \n",
    "\n",
    "- **ì „ì²´ êµ¬ì¡°**  \n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png\" width=\"500\">\n",
    "\n",
    "- **ë°ìì–´ì…‹ í˜•íƒœ**\n",
    "step1) SFT(actor_training_data): SFT ì§€ë„ ë¯¸ì„¸ ì¡°ì •ì— ì‚¬ìš©ë˜ëŠ” JSON ë°ì´í„°\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\",\n",
    "        \"completion\": \"\"        \n",
    "    }, ...\n",
    "]\n",
    "```\n",
    "\n",
    "- **ê²°ê³¼ë¬¼**\n",
    "    - Before: ë‹¤ìŒ ë‹¨ì–´ë§Œ ì˜ ìƒì„± í–ˆì—ˆìŒ\n",
    "    - After: ì§ˆë¬¸ì— â€˜ì˜â€™ ëŒ€ë‹µí•˜ëŠ” ëª¨ë¸\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '/workspace/kogpt/kogpt-ft-3',\n",
    "    \"kakaobrain/kogpt\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "51S4uJpbSMVV"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4OzdlU9SMVV",
    "outputId": "a7c59a70-6051-40ea-837b-9a9fb32ff847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path_1_SFT='./data_kochatgpt/kochatgpt_1_SFT.jsonl', model_name=('/workspace/kogpt/kogpt-ft-3',), max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT-kogpt')\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_1_SFT', type=str, default='./data_kochatgpt/kochatgpt_1_SFT.jsonl')\n",
    "parser.add_argument('--model_name', type=str, default='kogpt')\n",
    "parser.add_argument('--max_epochs', type=int, default=2)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--output_dir', type=str, default='./output_1_SFT-kogpt')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.model_name = '/workspace/kogpt/kogpt-ft-3',\n",
    "\n",
    "args.max_epochs = 2\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498,
     "referenced_widgets": [
      "3532edab6de845b589a83f395917578f",
      "9e1646bc324c40428c9fe9b7ab8a6921",
      "1c2de103392e48ba954c8df37612b395",
      "56fca42756f94eabaf830bc86d18c4a7",
      "4cb558fcf41c4406b15c98af203bf272",
      "95adfba856d54c50a4e1d71b8b4a3308",
      "2e3e77f407bc48279570721105a621ed",
      "975c6155ea2c476684ea2b6df5483e1b",
      "c20354dbe20844c28fe1455c765db2f6",
      "13035ae8dc2741a8bd0a96c234300af2",
      "3c309d97eb5d43c798d6db8f49a8e434",
      "1d0e339ffba14e8d91368770385f1db9",
      "9578bba68cfa4c03a21d6251c20053d5",
      "36d0515d2c2847868b3d9dd2ea05f25a",
      "bbd8f769a05048bca99d1075bfe47b6e",
      "fa47d352268a4be388b820f9a4a6c5a5",
      "0ecd2ec82a804860a78885c97fc250b2",
      "c677b05ae99841c6879009aa20f2e5f2",
      "f541cd5c2c704129855a39d1c81a48b3",
      "b7b37f9f9d764172828cb005d2b9f633",
      "d50332475bbe429f9ef61ba26609cdc6",
      "78ff1615fa4b4d8ab51c4683a43f4182",
      "a6fc9ca6dd874b3b80fe9aa732ec5d69",
      "cab874fdbbc0434f876661f578f3f855",
      "59d060b9aa65450c8828de299b2b3498",
      "930bd459e2794a32ba4c303b31aaf159",
      "b887240c83db4b77b8bb8391ba0364b4",
      "daf3185604a54f268a5b5f3a456c0f01",
      "7a12232f55c5447fb0284ca001cc0023",
      "674175399071436b8da7cd07ebaa29c8",
      "9c03b82f323e45dbaab32148f00a6938",
      "7180bc85cb0b4c20b65365c5e84767d4",
      "93479918dcbd40df9a733045e116a90b"
     ]
    },
    "id": "2SrHOwC-SMVW",
    "outputId": "f0e5ab01-d6be-4b69-937a-7436faad91fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ã¬Ä·ÄªÃ«Ä§Ä·', 'Ã­Ä·Äº', 'Ã¬Ä¦Â¸Ã¬Ä¼Ä¶', '.', 'Ä Ã­Ä·Ä¾ÃªÂµÅƒÃ¬Ä¸Â´', 'Ä GP', 'T', '-', '2', 'Ä Ã¬Å€Ä§Ã«Ä­ÄªÃ«Ä­Â¤', '.', 'ğŸ˜¤', ':)', 'l^o']\n"
     ]
    }
   ],
   "source": [
    "## test & load skt gpt2 kroean\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"kakaobrain/kogpt\",\n",
    "        revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))\n",
    "# ['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     './kogpt-ft-3',\n",
    "    \"kakaobrain/kogpt\",\n",
    "    revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    # torch_dtype='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498,
     "referenced_widgets": [
      "3532edab6de845b589a83f395917578f",
      "9e1646bc324c40428c9fe9b7ab8a6921",
      "1c2de103392e48ba954c8df37612b395",
      "56fca42756f94eabaf830bc86d18c4a7",
      "4cb558fcf41c4406b15c98af203bf272",
      "95adfba856d54c50a4e1d71b8b4a3308",
      "2e3e77f407bc48279570721105a621ed",
      "975c6155ea2c476684ea2b6df5483e1b",
      "c20354dbe20844c28fe1455c765db2f6",
      "13035ae8dc2741a8bd0a96c234300af2",
      "3c309d97eb5d43c798d6db8f49a8e434",
      "1d0e339ffba14e8d91368770385f1db9",
      "9578bba68cfa4c03a21d6251c20053d5",
      "36d0515d2c2847868b3d9dd2ea05f25a",
      "bbd8f769a05048bca99d1075bfe47b6e",
      "fa47d352268a4be388b820f9a4a6c5a5",
      "0ecd2ec82a804860a78885c97fc250b2",
      "c677b05ae99841c6879009aa20f2e5f2",
      "f541cd5c2c704129855a39d1c81a48b3",
      "b7b37f9f9d764172828cb005d2b9f633",
      "d50332475bbe429f9ef61ba26609cdc6",
      "78ff1615fa4b4d8ab51c4683a43f4182",
      "a6fc9ca6dd874b3b80fe9aa732ec5d69",
      "cab874fdbbc0434f876661f578f3f855",
      "59d060b9aa65450c8828de299b2b3498",
      "930bd459e2794a32ba4c303b31aaf159",
      "b887240c83db4b77b8bb8391ba0364b4",
      "daf3185604a54f268a5b5f3a456c0f01",
      "7a12232f55c5447fb0284ca001cc0023",
      "674175399071436b8da7cd07ebaa29c8",
      "9c03b82f323e45dbaab32148f00a6938",
      "7180bc85cb0b4c20b65365c5e84767d4",
      "93479918dcbd40df9a733045e116a90b"
     ]
    },
    "id": "2SrHOwC-SMVW",
    "outputId": "f0e5ab01-d6be-4b69-937a-7436faad91fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ê·¼ìœ¡ì— ì§€ì†ì ì¸ ìê·¹(stimulation)ì„ ì£¼ì–´ì•¼ í•œë‹¤. ì¦‰, ê·¼ë ¥ ìš´ë™ì€ ê·¼ì„¬ìœ ë¥¼ ì†ìƒì‹œí‚¤ê³  ê·¸ í›„ íšŒë³µë˜ì–´ ì›ìƒíƒœë¡œ ëŒì•„ê°€ëŠ”ë° í•„ìš”í•œ ì‹œê°„ë³´ë‹¤ ë” ë§ ì€ ê¸°ê°„ ë™ì•ˆ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ë§Œ íš¨ê³¼ê°€ ìˆë‹¤\"ë¼ê³  ê°•ì¡°í–ˆìŠµë‹ˆë‹¤.. ë˜í•œ \"ê·¼ë ¥ìš´ë™ì˜ ëª©ì ì¸ 'ìµœëŒ€ì‚°ì†Œì„­ì·¨ëŸ‰' ì¦ê°€ì™€ ê°™ ì´ ì¤‘ìš”í•˜ê²Œ ìƒê°í•  ê²ƒë“¤ì´ ëª‡ ê°€ì§€ìˆëŠ”ë°ìš”~ ì²«ì§¸! ìœ ì‚°ì†Œì„± ëŠ¥ë ¥ í–¥ìƒ ë‘˜ì§¸!! ì²´ì§€ë°© ê°ì†Œ ì…‹ì§¸!!! ê´€ì ˆ ê±´ê°• ìœ ì§€ ë„·ì§¸!!!! ìœ ì—°ì„±ê³¼ ê· í˜•ê°ê° ì¦ì§„ ë‹¤ì„¯ì§¸!!!!!!!! ì‹¬íì§€êµ¬ ë ¥ê³¼ íí™œëŸ‰ì˜ ê°œì„  ì—¬ì„¯ì§¸????? ì‹ ì²´êµ¬ì„± ë³€í™”ì…ë‹ˆë‹¤...^^ ìœ„ì—ì„œë„ ì–¸ê¸‰ í–ˆë“¯ì´ ìµœëŒ€ ì‚°ì†Œ ì„­ì·¨ëŸ‰ë„ ë§¤ìš°ì¤‘ìš”í•˜ì§€ë§Œ ê·¸ê²ƒë§Œí¼ì´ë‚˜ ë˜ ë‹¤ë¥¸ ìš”ì†Œë“¤ ë„ ê³ ë ¤í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.... ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì—¬ì„¯ë²ˆì§¸ê¹Œì§€ ê°€ì„œ ê°€ì¥ í¬ ê²Œ ì‹ ê²½ì¨ì£¼ì‹œë©´ ì¢‹ìœ¼ì‹¤ ë¶€ë¶„ì¸ë°ìš”~~ ë°”ë¡œ ì²´ì„±ë¶„ë³€í™” ì…ë‹ˆë‹¤........ì²´ ì„±ë¶„ì´ë€ ì§€ë°©ì¡°ì§ + ë‹¨ë°±ì§ˆ ì¡°ì§ ìœ¼ë¡œ êµ¬ì„±ëœê²ƒì¸ë° ì´ê²ƒë“¤ì˜ ë¹„ìœ¨ (% body fat mass / ì²´ì¤‘kgë‹¹ ë¶€í”¼ kg/m3 ë“± ì„ ë§í•©ë‹ˆë‹¤ ^^; ì‰½ì£ ??á„ ) ì— ë”°ë¼ ëª¸ë§¤ë‚˜ ëª¸ì§± ì—¬ë¶€ë“± ì—¬ëŸ¬ê°€ì§€ ê²°ê³¼ ë¥¼ ë³¼ ìˆ˜ ì‡ë‹µë‹ˆë‹¤ á„’ ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼\n"
     ]
    }
   ],
   "source": [
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=256,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)\n",
    "\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "# generation_args = dict(\n",
    "#     num_beams=4,\n",
    "#     repetition_penalty=2.0,\n",
    "#     no_repeat_ngram_size=4,\n",
    "#     eos_token_id=375, # \\n\n",
    "#     max_new_tokens=64,\n",
    "#     do_sample=True,\n",
    "#     top_k=50,\n",
    "#     early_stopping=True\n",
    "# )\n",
    "# generator(\n",
    "#     [\"0 : **ëŠ” ê²Œì„ ì¢‹ì•„í•˜ë‹ˆ\\n1 :\",\n",
    "#     \"0 : ì–´ì œ ê°•ë‚¨ì—ì„œ ì‚´ì¸ì‚¬ê±´ ë‚¬ëŒ€ ã…œã…œ ë„ˆë¬´ ë¬´ì„œì›Œ\\n1 : í— ì™œ? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?\\n0 : ì‚¬ì§„ë³´ë‹ˆê¹Œ ë§‰ í”¼í˜ë¦¬ëŠ” ì‚¬ëŒìˆê³  ê²½ì°°ë“¤ì´ ë– ì„œ ì œì••í•˜ê³  ë‚œë¦¬ë„ ì•„ë‹ˆì—ˆë‹¤ë˜ë°??\\n1 :\",\n",
    "#     \"0 : ìê¸°ì•¼ ì–´ì œëŠ” ë‚˜í•œí…Œ ì™œ ê·¸ë¬ì–´?\\n1 : ë­” ì¼ ìˆì—ˆì–´?\\n0 : ì–´ë–»ê²Œ ë‚˜í•œí…Œ ë§ë„ ì—†ì´ ê·¸ëŸ´ ìˆ˜ ìˆì–´? ë‚˜ ì§„ì§œ ì‹¤ë§í–ˆì–´\\n1 : \"],\n",
    "#     **generation_args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "NXT_zkuwSMVX"
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "# DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "# DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
    "        \"Write a detailed response that appropriately completes the request.\\n\\n\"\\\n",
    "        \"### Instruction:\\n{prompt}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\\n\"\n",
    "        \"Write a detailed response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_a_EbaW8SMVX",
    "outputId": "b41c3a6f-381c-4544-e041-3c29e7fcb28e"
   },
   "outputs": [],
   "source": [
    "# ## ëª¨ë¸ ì¤€ë¹„\n",
    "# model = AutoModelForCausalLM.from_pretrained(args.model_name).to('cuda')\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#     args.model_name,\n",
    "#     padding_side=\"right\",\n",
    "#     model_max_length=512,    \n",
    "# )\n",
    "# tokenizer.add_special_tokens(\n",
    "#     {\n",
    "#         \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "#         \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "#         \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "#     }\n",
    "# )    \n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     3,
     105
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E6u3j_YSMVX",
    "outputId": "4beba0f2-5a50-4d6e-8380-9c30217c7916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "## prepare data\n",
    "from typing import Optional, Dict, Sequence\n",
    "    \n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        \n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_input = 'input'  # ë‚´ ë°ì´í„°ì—” inputì´ ì—†ë‹¤\n",
    "        pattern_output = 'completion'  # output\n",
    "\n",
    "        ############################################################\n",
    "        ## load dataset\n",
    "        # ë‚´ ë°ì´í„°ì…‹ì—” inputì´ ì—†ë‹¤\n",
    "#         data_path_1_SFT = 'data_kochatgpt/korean_chatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "        # {'prompt': 'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "        #  'completion': \"'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë©°, ì§ì ‘ì ìœ¼ë¡œ ì‹í’ˆì— ê´€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” í•œìš°, ì‡ ê³ ê¸°, ë¼ì§€ê³ ê¸° ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•œìš°ëŠ” ëŒ€í‘œì ì¸ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì—, í•œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ì•ŒëŸ¬ì§€ë‚˜ ê°œë³„ ê±´ê°• ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ í›„ì— ì„ íƒí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\",\n",
    "        #  'tokens': 193}        \n",
    "\n",
    "        ############################################################\n",
    "        ## ë°ì´í„°ì…‹ ë§Œë“¤ê¸°, sourceì™€ target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "        # ì…ë ¥\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # ì¶œë ¥\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        ############################################################\n",
    "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # sourceë§Œ\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## ì…ë ¥ì€ source, ì¶œë ¥ì€ source+target ì´ì§€ë§Œ í•™ìŠµì€ target ë¶€ë¶„ë§Œ\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source ë¶€ë¶„ì€ -100ìœ¼ë¡œ ì±„ìš´ë‹¤\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
    "        \n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
    "        \n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
    "eval_dataset  = None  # evalì€ ì•ˆí•¨\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "# check\n",
    "# print('input : %s'%train_dataset.input_ids[0])\n",
    "# print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "Uuh3PgkLSMVY",
    "outputId": "5509fe61-a95b-4942-f701-26327662fb85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">26</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>eval_dataset=eval_dataset,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>26 trainer.train()                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 # trainer.save_state()</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 # safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)</span>               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   â”‚   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2717</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2714 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2715 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2716 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>2717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2718 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2719 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2720 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">488</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 488 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 491 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">197</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">194 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>197 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.64</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.05</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49.50</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.42</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated \n",
       "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m26\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2mâ”‚   \u001b[0meval_dataset=eval_dataset,                                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m24 \u001b[0m)                                                                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m26 trainer.train()                                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m# trainer.save_state()\u001b[0m                                                                      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m# safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)\u001b[0m               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m29 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m)                                                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1662 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0margs=args,                                                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mtrial=trial,                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1929 \u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m                                                                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   â”‚   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2717\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2714 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2715 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2716 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m2717 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0mloss.backward()                                                               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2718 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2719 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2720 \u001b[0m                                                                                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m488\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mcreate_graph=create_graph,                                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0minputs=inputs,                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 487 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m)                                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 488 \u001b[2mâ”‚   â”‚   \u001b[0mtorch.autograd.backward(                                                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 490 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m)                                                                                 \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 491 \u001b[0m                                                                                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m197\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m197 \u001b[2mâ”‚   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m128.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m23.64\u001b[0m GiB total capacity; \u001b[1;36m22.05\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m49.50\u001b[0m MiB free; \u001b[1;36m22.42\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated \n",
       "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## í•™ìŠµ (10min)\n",
    "# training_args ìˆ˜ì • ê°€ëŠ¥: https://github.com/Beomi/KoAlpaca/blob/main/train.sh ì°¸ê³ \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    gradient_accumulation_steps=8,\n",
    "#     per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "#     eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    save_total_limit = 1,\n",
    "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    optim='adafactor',\n",
    "    report_to='none'\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_state()\n",
    "# safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output_1_SFT'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7CFw58BSMVZ",
    "outputId": "b6f8d7ec-ace1-438b-e0f0-d66036e1ff6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ë¶ˆê³ ê¸°ë¥¼ ë¨¹ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” ê±´ê°•ì— ì¢‹ì€ ì‹í’ˆ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê±´ê°•í•œ ì‹ìŠµê´€ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\\n\\në”°ë¼ì„œ ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶ˆê³ ê¸°ë¥¼ ì›í•˜ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„\n",
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 47ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. Canadd of Johnson. There are personal service, but I can assist you would you like to provide more context or\n",
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤í—¤ì´ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ìƒŒí”„ë€ì‹œìŠ¤ì½”ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. Communicate of Capability, I am not have service to provide more context or details. Could you please prov\n",
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì´ë¯€ë¡œ, ë¯¸ì„¸ë¨¼ì§€ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë¯¸ì„¸ë¨¼ì§€ ë°œìƒ ì›ì¸ì€ ë‹¤ì–‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í˜¸í¡ê¸° ì§ˆí™˜, ì‹¬í˜ˆê´€ê³„ ì§ˆí™˜, ì•Œë ˆë¥´ê¸° ì§ˆí™˜ ë“±ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ë¯¸ì„¸ë¨¼ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì‹œë ¤ë©´ í•´ë‹¹ ì¥ì†Œì˜ ê³µê¸°ì²­ì •ê¸° ë˜ëŠ” ë§ˆìŠ¤í¬ë¥¼ ì°©ìš©í•˜ì‹œê¸° ë°”\n"
     ]
    }
   ],
   "source": [
    "## ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
    "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
    "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print(('#'*70))\n",
    "    print(('completion: %s'%(result[0]['generated_text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uZXNbXG8v4g5"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti9X13R6SMVZ"
   },
   "source": [
    "#### GPT2 ëª¨ë¸ì´ ì‚¬ëŒì˜ ì§ˆë¬¸ì— ëŒ€í•´ **'ì˜'** ëŒ€ë‹µí•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.  \n",
    "#### ``output_1_SFT`` í´ë”ì— í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
    "#### ì´ì œ step2) RM ë³´ìƒëª¨ë¸ì„ í•™ìŠµí•´ ë³¼ê¹Œìš”??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V7EindF6SMVZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tOV7g49CSMVZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU4djUZ_7tit"
   },
   "source": [
    "### 2) RM: ì¢‹ì€ ê¸€ ì±„ì ê¸° ë§Œë“¤ê¸°\n",
    "- Collect a human annotated dataset and train a reward model\n",
    "- **ë°°ê²½**\n",
    "    - ê¸°ì¡´ AIëŠ” ì£¼ê´€ì ì¸ ê¸€ì„ ì±„ì (ì ìˆ˜í™”) í•  ìˆ˜ ì—†ì—ˆìŒ\n",
    "    - ì‚¬ëŒì´ ì§ì ‘ í”¼ë“œë°±ì„ ì¤˜ì„œ ê¸€ ì±„ì ì˜ ì²™ë„ë¡œ ì‚¬ìš©í•˜ì\n",
    "    - ë§¤ë²ˆ ì‚¬ëŒì´ ì±„ì í•  ìˆ˜ ì—†ìœ¼ë‹ˆ, ì‚¬ëŒì˜ ì±„ì ì„ ëª¨ë°©í•˜ëŠ” **ì¢‹ì€ê¸€ ì±„ì  AIëª¨ë¸** ì„ ë§Œë“¤ì\n",
    "\n",
    "    - ì±„ì  AIëª¨ë¸ì„ ë§Œë“œë ¤ë©´, ì‚¬ëŒì´ ê¸€ì„ ì±„ì í•œ ë°ì´í„°ì…‹(33,000ê°œ)ì´ í•„ìš”í•˜ë‹¤\n",
    "    - ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•´ AIëª¨ë¸ì´ ìƒì„±í•œ ì—¬ëŸ¬ ê¸€(í•œ ë²ˆì— 4~6ê°œ ì„¸íŠ¸)ì„ ì‚¬ëŒì´ ì§ì ‘ rankingì„ ë§¤ê¸´ë‹¤.\n",
    "    - ì™œ?? ì‚¬ëŒì´ ìƒì„±í•œ ê¸€ì— ë°”ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê²Œ ë˜ë©´ ì‚¬ëŒë§ˆë‹¤ ê¸°ì¤€ì´ ë‹¤ë¥¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ìˆœìœ„ë¡œ\n",
    "    - **C > B > A**  \n",
    "\n",
    "- **Human labeling ì˜ˆì‹œ**\n",
    "<img src=\"img/2_RM_1.png\" width=\"700\">  \n",
    "\n",
    "\n",
    "- **ì¢‹ì€ê¸€ ì±„ì  ëª¨ë¸ í•™ìŠµ(RM, Reward Model)**\n",
    "    - 1ë“± ê¸€ì€ ë†’ì€ ì ìˆ˜ë¥¼\n",
    "    - ê¼´ë“± ë°ì´í„°ëŠ” ë‚®ì€ ì ìˆ˜ë¥¼\n",
    "    - ì…ë ¥: AIê°€ ìƒì„±í•œ ê¸€\n",
    "    - ì¶œë ¥: 0~1ì   \n",
    "\n",
    "\n",
    "- ë³´ìƒëª¨ë¸ ì…ì¶œë ¥\n",
    "<img src=\"img/2_RM_2.png\" width=\"700\">\n",
    "\n",
    "- **ê²°ê³¼ë¬¼**\n",
    "    - Before: ì¢‹ì€ ê¸€, ë‚˜ìœ ê¸€ íŒë‹¨ ë¶ˆê°€ëŠ¥\n",
    "    - After: ì‚¬ëŒì´ ì½ê¸°ì— ì¢‹ì€ê¸€/ë‚˜ìœê¸€ íŒë‹¨ ëª¨ë¸\n",
    "    \n",
    "    \n",
    "- **ì „ì²´ êµ¬ì¡°**\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuXE_BLP8ALx"
   },
   "source": [
    "##### Step 2) Train the reward model\n",
    "-[ref](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/train_reward_model.py)\n",
    "\n",
    "We use rm-static as dataset to train our reward model.\n",
    "It is a dataset of chosen & rejected response of the same prompt.\n",
    "You can download the dataset from huggingface automatically.\n",
    "Use these code to train your reward model.\n",
    "\n",
    "##### Naive reward model training\n",
    "python train_reward_model.py --pretrain <your model path>\n",
    "\n",
    "##### if to use LoRA\n",
    "python train_reward_model.py --pretrain <your model path> --lora_rank 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "KphqHc6lgu38"
   },
   "outputs": [],
   "source": [
    "# ## setup(1min)\n",
    "# # torch ë²„ì „ ë‹¤ìš´. torch>=2.0 ì—ì„  colosalaiê°€ ë™ì‘ì•ˆí•¨\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"Torch version:{}\".format(torch.__version__))\n",
    "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "# # for ColossalAI\n",
    "# !pip install colossalai==0.2.7\n",
    "\n",
    "# # setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiuVfmvb7tTD",
    "outputId": "fc7095f0-b5dc-4bfe-bccb-0b6a3b24d383"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
      "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: Meta\n",
      "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
      "       new kernel: registered at /dev/null:241 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
      "  self.m.impl(name, dispatch_key, fn)\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import argparse\n",
    "\n",
    "import loralib as lora\n",
    "import torch\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMRM\n",
    "from chatgpt.models.gpt import GPTRM\n",
    "from chatgpt.models.opt import OPTRM\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNEweAK77tNT",
    "outputId": "ccd6fbec-82d8-4e53-800c-40b2a1cd29f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(output_dir='./output_2_RM', data_path_2_RM='./data_kochatgpt/kochatgpt_2_RM.jsonl', strategy='naive', model='gpt2', pretrain_model='./output_1_SFT', pretrain_tokenizer='skt/kogpt2-base-v2', dataset='Dahoas/rm-static', save_path='rm_ckpt.pth', max_epochs=3, batch_size=4, lora_rank=0, max_len=512, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n",
    "parser.add_argument('--data_path_2_RM', type=str, default='./data_kochatgpt/kochatgpt_2_RM.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain_model', type=str, default=None)\n",
    "parser.add_argument('--pretrain_tokenizer', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
    "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_len', type=int, default=512)  # wygo ì¶”ê°€\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.max_epochs = 3\n",
    "args.pretrain_tokenizer = 'skt/kogpt2-base-v2'  # pretrained í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°\n",
    "# args.pretrain_model = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "args.pretrain_model = './output_1_SFT'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (sft ê·¸ëŒ€ë¡œ ì¨ì•¼ í•¨)\n",
    "args.verbose = True\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "mtwPLUIq7tIb"
   },
   "outputs": [],
   "source": [
    "# configure strategy\n",
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [],
    "id": "oZ96VsvwnHG9"
   },
   "outputs": [],
   "source": [
    "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
    "from typing import Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "# from ..base import RewardModel\n",
    "from chatgpt.models.base import RewardModel\n",
    "\n",
    "\n",
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT Reward model.\n",
    "    Args:\n",
    "        pretrained (str): Pretrained model name or path.\n",
    "        config (GPT2Config): Model config.\n",
    "        checkpoint (bool): Enable gradient checkpointing.\n",
    "        lora_rank (int): Rank of the low-rank approximation.\n",
    "        lora_train_bias (str): LoRA bias training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        \n",
    "        # model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        # ì¶”ê°€, 230421    \n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "        \n",
    "    # ì¶”ê°€, 230421, config.jsonì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì¶”ê°€\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ee3we4uB8EdK",
    "outputId": "fe1726bf-1a5a-4368-c793-1fee2d9c95bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at ./output_1_SFT were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    # load pretrained gpt2    \n",
    "    if args.model == 'gpt2':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain_tokenizer, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = GPTRM_custom(pretrained=args.pretrain_model, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
    "\n",
    "    elif args.model == 'bloom':\n",
    "        model = BLOOMRM(pretrained=args.pretrain_model, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain_tokenizer)\n",
    "    \n",
    "    elif args.model == 'opt':\n",
    "        model = OPTRM(pretrained=args.pretrain_model, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "    \n",
    "    \n",
    "    # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GD94Zd-b7tDL",
    "outputId": "3a5828e5-3ef5-47df-87ad-2f26d1748663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## data check ##\n",
      "{'prompt': 'Below is an instruction that describes a task.\\nì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\nWrite a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n### Instruction(ëª…ë ¹ì–´):\\në²ˆë””ëŠ” ìì‹ ì´ íƒì •ì¡ì§€, ë²”ì£„ì†Œì„¤ ê·¸ë¦¬ê³  ì„±ë²”ì£„ ê´€ë ¨ ì‹¤ì œ ë²”ì£„ ë‹¤íë©˜í„°ë¦¬ë“¤ì„ íƒë…í–ˆë‹¤ê³  ëˆ„êµ¬ì—ê²Œ ë§í–ˆë‚˜?\\n\\n### Response(ì‘ë‹µ):', 'completion_0': 'Allow me to answer your question. I know that you are curious about me.', 'completion_1': 'ë²ˆë””ëŠ” ë‹¤ì–‘í•œ ì¸í„°ë·°ìë“¤ê³¼ ë‰´ìŠ¤í™ë³´ ë‹´ë‹¹ìë“¤ê³¼ì˜ ë©´ë‹´ ë•Œ ë°í˜”ë‹¤.', 'completion_2': 'ë¼ì´ì–¸ì—ê²Œ ë§í–ˆë‹¤.', 'ranking': [2, 1, 0]}\n",
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': 'Below is an instruction that describes a task.\\nì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\nWrite a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n### Instruction(ëª…ë ¹ì–´):\\nì• í”Œì€ ë¦¬ì‚¬ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í–ˆì–´\\n\\n### Response(ì‘ë‹µ):', 'chosen': 'ì• í”Œì´ ëˆ„êµ¬ì¸ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ì—†ì–´ì„œ, ë¦¬ì‚¬ê°€ ëˆ„êµ¬ì¸ì§€ì™€ ì–´ë–¤ ìƒí™©ì—ì„œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'rejected': 'ì• í”Œì€ ë¦¬ì‚¬ë¥¼ ìœ„í•´ ê³ ê° ì„œë¹„ìŠ¤ ë¶€ì„œì—ì„œ ê³ ê° ë‹¤ì–‘í•œ ì»´í“¨í„° ê´€ë ¨ ë¬¸ì œì— ëŒ€í•´ ì‘ë‹µí•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ì§€ì›ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ í•˜ë“œì›¨ì–´ ë¬¸ì œë¥¼ ê²½í—˜í•  ë•Œ, ì „ë¬¸ê°€ë“¤ì€ í•„ìš”í•œ ìˆ˜ë¦¬(ìˆ˜ë¦¬, ì¶”ê°€ ë¶€í’ˆ ì œê³µ, ì†Œí”„íŠ¸ì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ ë“±)ì„ ì œê³µí•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ìš©ìê°€ ì‚¬ìš© ë°©ë²• ë¬¸ì œë‚˜ ê¸°íƒ€ ë¬¸ì œë¥¼ ê²½í—˜í•  ë•Œ, ëŒ€í™” ìƒëŒ€ë¡œ ì‚¬ìš©ìë¥¼ ì§€ì›í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ ê³ ê° ì„œë¹„ìŠ¤ ì§ì›ë“¤ì´ ì‚¬ìš©ìì—ê²Œ ìƒë‹´í•˜ê³  ë„ì›€ì„ ì£¼ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ì¸í„°ë„·ì—ì„œ ì œê³µë˜ëŠ” ì •ë³´ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê±°ë‚˜ ê³ ê° ì„œë¹„ìŠ¤ ì›¹ ì‚¬ì´íŠ¸ë¥¼ í†µí•´ ìì‹ ì˜ ë¬¸ì œë¥¼ ì§„ë‹¨í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë¦¬ì‚¬ë¥¼ ì²˜ë¦¬í•´ ì™”ìŠµë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# make ranking data to chosen, rejetced data\n",
    "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    \n",
    "    # lsw add\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "    list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "    for x,y in zip(list_data_dict, list_prompt):\n",
    "        x.update({'prompt':y})\n",
    "\n",
    "    if args.verbose:\n",
    "        print('## data check ##')\n",
    "        print((list_data_dict[0]))\n",
    "        \n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # data 1) 0 VS 1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    # data 2) 0 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # data 1) 1 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWzG_w3d7s9h",
    "outputId": "05ce4b31-5f97-4cd1-a0d1-8ae1b7e4f326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Below is an instruction that describes a task.\\nì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\nWrite a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n### Instruction(ëª…ë ¹ì–´):\\nìœ ì•„ì¸ì´ ë¥˜ìŠ¹ì™„ ê°ë…ì„ ë§Œë‚˜ ì˜í™” ë² í…Œë‘ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°›ì•˜ë˜ ê³³ì€?\\n\\n### Response(ì‘ë‹µ):', 'chosen': 'ìœ ì•„ì¸ì´ ë¥˜ìŠ¹ì™„ ê°ë…ì„ ë§Œë‚˜ ì˜í™” ë² í…Œë‘ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°›ì•˜ë˜ ê³³ì€ ë¥˜ìŠ¹ì™„ì˜ ì‚¬ë¬´ì‹¤ì…ë‹ˆë‹¤.', 'rejected': 'ëŒ€êµ¬ ì˜í™”ì‚¬ì˜¥'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1229.50it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 1372.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì²´í¬ì¸ ë˜ë‚˜ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):\n",
      "######################################################################\n",
      "## chosen ##\n",
      "ì œê°€ AI ì±—ë´‡ì´ê¸° ë•Œë¬¸ì— í˜¸í…”ì´ë‚˜ í•­ê³µí¸ ë“±ìœ¼ë¡œ ì–´ë–¤ ì²´í¬ì¸ì„ ë§ì”€í•˜ì‹œëŠ” ê²ƒì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œë©´ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "ë‹¤ì‹œ í•œë²ˆ ê°€ì§€ê²Œì„ì´ì§€ ì•Šì•„ ê°€ì§€ê²Œì„ì´ì§€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare for data and dataset\n",
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 29000 í•™ìŠµ\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 1000ê°œë§Œ í‰ê°€\n",
    "\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "OQI5a1hm7s3L"
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "if args.strategy.startswith('colossalai'):\n",
    "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
    "else:\n",
    "    optim = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "97mbryrTgXJ0"
   },
   "outputs": [],
   "source": [
    "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
    "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=strategy,\n",
    "                             optim=optim,\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             max_epochs=args.max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE6HOKdmgY6X"
   },
   "source": [
    "###### ì°¸ê³ \n",
    "- [train reward model](https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/trainer/rm.py#L68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpOYBhbH8HLh",
    "outputId": "32806121-5413-43fc-ea30-15afc3a940c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 1/25 [00:00<00:23,  1.04it/s]\u001b[A\n",
      "Train step of epoch 0:   4%|â–         | 1/25 [00:00<00:23,  1.04it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 2/25 [00:01<00:13,  1.69it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:   8%|â–Š         | 2/25 [00:01<00:13,  1.69it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 3/25 [00:01<00:09,  2.21it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  12%|â–ˆâ–        | 3/25 [00:01<00:09,  2.21it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:08,  2.59it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:08,  2.59it/s, loss=0.47] \u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–ˆ        | 5/25 [00:02<00:07,  2.85it/s, loss=0.47]\u001b[A\n",
      "Train step of epoch 0:  20%|â–ˆâ–ˆ        | 5/25 [00:02<00:07,  2.85it/s, loss=0.266]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 6/25 [00:02<00:06,  3.04it/s, loss=0.266]\u001b[A\n",
      "Train step of epoch 0:  24%|â–ˆâ–ˆâ–       | 6/25 [00:02<00:06,  3.04it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:02<00:05,  3.17it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:02<00:05,  3.17it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:03<00:05,  3.27it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:03<00:05,  3.27it/s, loss=2.69] \u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:03<00:04,  3.34it/s, loss=2.69]\u001b[A\n",
      "Train step of epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:03<00:04,  3.34it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:03<00:04,  3.38it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:03<00:04,  3.38it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.41it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.41it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:04<00:03,  3.44it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:04<00:03,  3.44it/s, loss=0.54] \u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:04<00:03,  3.45it/s, loss=0.54]\u001b[A\n",
      "Train step of epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:04<00:03,  3.45it/s, loss=0.351]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.46it/s, loss=0.351]\u001b[A\n",
      "Train step of epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.46it/s, loss=0.831]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:05<00:02,  3.47it/s, loss=0.831]\u001b[A\n",
      "Train step of epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:05<00:02,  3.47it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:05<00:02,  3.48it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:05<00:02,  3.48it/s, loss=1.99] \u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  3.48it/s, loss=1.99]\u001b[A\n",
      "Train step of epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  3.48it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  3.48it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  3.48it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:06<00:01,  3.49it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:06<00:01,  3.49it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.49it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.49it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.49it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.49it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:07<00:00,  3.49it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:07<00:00,  3.49it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.49it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.49it/s, loss=1.2]  \u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.49it/s, loss=1.2]\u001b[A\n",
      "Train step of epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.49it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.49it/s, loss=0.767]\u001b[A\n",
      "Train epoch:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:08<00:17,  8.57s/it],  3.49it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:08<00:00,  2.92it/s, loss=0.596, dist_mean=0.282]\u001b[A\n",
      "\n",
      "Train step of epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 1:   4%|â–         | 1/25 [00:00<00:06,  3.62it/s]\u001b[A\n",
      "Train step of epoch 1:   4%|â–         | 1/25 [00:00<00:06,  3.62it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 1:   8%|â–Š         | 2/25 [00:00<00:06,  3.54it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 1:   8%|â–Š         | 2/25 [00:00<00:06,  3.54it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 1:  12%|â–ˆâ–        | 3/25 [00:00<00:06,  3.51it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 1:  12%|â–ˆâ–        | 3/25 [00:00<00:06,  3.51it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 1:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:05,  3.50it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 1:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:05,  3.50it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 1:  20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.50it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 1:  20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.50it/s, loss=0.46] \u001b[A\n",
      "Train step of epoch 1:  24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.50it/s, loss=0.46]\u001b[A\n",
      "Train step of epoch 1:  24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.50it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 1:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.49it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 1:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:02<00:05,  3.49it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.49it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.49it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.49it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 1:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.49it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.48it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.48it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.47it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.47it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.47it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.47it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.47it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 1:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.47it/s, loss=0.121]\u001b[A\n",
      "Train step of epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.47it/s, loss=0.121]\u001b[A\n",
      "Train step of epoch 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.47it/s, loss=0.984]\u001b[A\n",
      "Train step of epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:02,  3.48it/s, loss=0.984]\u001b[A\n",
      "Train step of epoch 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:02,  3.48it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:02,  3.47it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:02,  3.47it/s, loss=1.81] \u001b[A\n",
      "Train step of epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:04<00:02,  3.47it/s, loss=1.81]\u001b[A\n",
      "Train step of epoch 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:04<00:02,  3.47it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  3.48it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 1:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  3.48it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:01,  3.48it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:01,  3.48it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:05<00:01,  3.48it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:05<00:01,  3.48it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.48it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.48it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.48it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.48it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:06<00:00,  3.48it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:06<00:00,  3.48it/s, loss=0.961]\u001b[A\n",
      "Train step of epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:06<00:00,  3.48it/s, loss=0.961]\u001b[A\n",
      "Train step of epoch 1:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:06<00:00,  3.48it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.48it/s, loss=0.383]\u001b[A\n",
      "Train epoch:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:16<00:08,  8.16s/it],  3.48it/s, loss=0.297]\u001b[A\n",
      "Train step of epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.18it/s, loss=0.508, dist_mean=0.715]\u001b[A\n",
      "\n",
      "Train step of epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 2:   4%|â–         | 1/25 [00:00<00:06,  3.60it/s]\u001b[A\n",
      "Train step of epoch 2:   4%|â–         | 1/25 [00:00<00:06,  3.60it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 2:   8%|â–Š         | 2/25 [00:00<00:06,  3.53it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 2:   8%|â–Š         | 2/25 [00:00<00:06,  3.53it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 2:  12%|â–ˆâ–        | 3/25 [00:00<00:06,  3.51it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 2:  12%|â–ˆâ–        | 3/25 [00:00<00:06,  3.51it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 2:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:06,  3.49it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 2:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:06,  3.49it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 2:  20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.49it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 2:  20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.49it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 2:  24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.48it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 2:  24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.48it/s, loss=0.923]\u001b[A\n",
      "Train step of epoch 2:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:02<00:05,  3.48it/s, loss=0.923]\u001b[A\n",
      "Train step of epoch 2:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:02<00:05,  3.48it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.48it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.48it/s, loss=0.317]\u001b[A\n",
      "Train step of epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.47it/s, loss=0.317]\u001b[A\n",
      "Train step of epoch 2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.47it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.47it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.47it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.47it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.47it/s, loss=0.223]\u001b[A\n",
      "Train step of epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.46it/s, loss=0.223]\u001b[A\n",
      "Train step of epoch 2:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.46it/s, loss=0.403]\u001b[A\n",
      "Train step of epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.46it/s, loss=0.403]\u001b[A\n",
      "Train step of epoch 2:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.46it/s, loss=0.0369]\u001b[A\n",
      "Train step of epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.47it/s, loss=0.0369]\u001b[A\n",
      "Train step of epoch 2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.47it/s, loss=0.484] \u001b[A\n",
      "Train step of epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:02,  3.47it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:02,  3.47it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:02,  3.47it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:02,  3.47it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:04<00:02,  3.47it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:04<00:02,  3.47it/s, loss=0.0221]\u001b[A\n",
      "Train step of epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  3.47it/s, loss=0.0221]\u001b[A\n",
      "Train step of epoch 2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  3.47it/s, loss=0.158] \u001b[A\n",
      "Train step of epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:01,  3.47it/s, loss=0.158]\u001b[A\n",
      "Train step of epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:01,  3.47it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:05<00:01,  3.47it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:05<00:01,  3.47it/s, loss=0.213]\u001b[A\n",
      "Train step of epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.47it/s, loss=0.213]\u001b[A\n",
      "Train step of epoch 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.47it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.47it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.47it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:06<00:00,  3.47it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:06<00:00,  3.47it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:06<00:00,  3.47it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 2:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:06<00:00,  3.47it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.47it/s, loss=0.143]\u001b[A\n",
      "Train epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  8.03s/it],  3.47it/s, loss=0.137]\u001b[A\n",
      "Train step of epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.17it/s, loss=0.751, dist_mean=0.689]\u001b[A\n",
      "Train epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  8.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# train!!\n",
    "trainer.fit(use_lora=args.lora_rank)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(optim,\n",
    "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)\n",
    "\n",
    "model.save_pretrained(args.output_dir)  # config.json ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaGv2_qUsRrb",
    "outputId": "721873d7-4ba5-49e3-b83b-fef5bd31e2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: í•œêµ­ì€ ëŒ€í•œë¯¼êµ­ ì…ë‹ˆë‹¤\n",
      "reward score: -8.1\n"
     ]
    }
   ],
   "source": [
    "# ë³´ìƒëª¨ë¸ ì²´í¬\n",
    "def inference_RM(input_text='ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "    \n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "    \n",
    "    return output_reward\n",
    "\n",
    "\n",
    "input_text = 'í•œêµ­ì€ ëŒ€í•œë¯¼êµ­ ì…ë‹ˆë‹¤'\n",
    "# input_text = 'ì¸ê³µì§€ëŠ¥ì€ ì¸ê³µì§€ëŠ¥ ì…ë‹ˆë‹¤'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Qt21yMWCv1S0"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-sht1EoAt1S"
   },
   "source": [
    "#### ì‚¬ëŒì˜ ì„ í˜¸ë„ë¥¼ ëª¨ë°©í•œ ì¢‹ì€ ê¸€ ì±„ì ê¸° ëª¨ë¸ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.  \n",
    "#### ``output_2_RM`` í´ë”ì— í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
    "#### ì´ì œ step3) PPO ëª¨ë¸ì„ í•™ìŠµí•´ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G_nNydSKAtOu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4IFUPxId7sa3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqkr7S_-pO9m"
   },
   "source": [
    "### Step 3) ì‚¬ëŒì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ í•™ìŠµ\n",
    "\n",
    "- Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (e.g. PPO)\n",
    "- ë°°ê²½\n",
    "    - **ì‚¬ëŒì˜ ìˆœìœ„ë¥¼ ëª¨ì‚¬í•œ ë³´ìƒëª¨ë¸(RM)** ì˜ ì ìˆ˜ê°€ ë†’ì•„ì§€ë„ë¡ í•™ìŠµ (31,000ê°œ)\n",
    "    - ì´ˆê¸° ëª¨ë¸ì— ë¹„í•´ ë„ˆë¬´ ë§ì´ ë°”ë€Œì§€ ì•Šë„ë¡  \n",
    "    \n",
    "    \n",
    "    \n",
    "<img src=\"./img/3_PPO_1.png\" width=\"650\">\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "EQd6DxzgpOdz",
    "outputId": "c50152fd-5fb1-4ca8-c224-3b38909fa235"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/oC7Cw3fu3gU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fb40f1ace50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human Feedback RL\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('oC7Cw3fu3gU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "m2ZFYClBpTol"
   },
   "source": [
    "##### RLHF\n",
    "- [train_dummy](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/train_dummy.py)\n",
    "- [train_prompts](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/train_prompts.py)\n",
    "- [ì¢‹ì€ì„¤ëª…](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093)\n",
    "\n",
    "- [dataset1](https://huggingface.co/datasets/Dahoas/rm-static)\n",
    "- [dataset2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/chatgpt.png\" width=\"500\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- Fine-tuning íƒœìŠ¤í¬ë¥¼ ê°•í™”í•™ìŠµ ë¬¸ì œë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •í˜•í™”\n",
    "    - Policy : ì–¸ì–´ëª¨ë¸ - í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í…ìŠ¤íŠ¸ì˜ ì‹œí€€ìŠ¤(í˜¹ì€ ê·¸ í™•ë¥ )ë¥¼ ë¦¬í„´\n",
    "    - Action space : ì–¸ì–´ëª¨ë¸ì˜ ëª¨ë“  ë‹¨ì–´ (ì¼ë°˜ì ìœ¼ë¡œ 5ë§Œê°œ ë¶„ëŸ‰)\n",
    "    - Observation space : ê°€ëŠ¥í•œ ì¸í’‹ í† í° ì‹œí€€ìŠ¤ (ë‹¨ì–´ê°œìˆ˜^ì‹œí€€ìŠ¤ê¸¸ì´ ì´ë¯€ë¡œ ì—„ì²­ í¼!)\n",
    "    - Reward function : ë³´ìƒëª¨ë¸ê³¼ policy shiftì— ëŒ€í•œ ì œì•½ì¡°ê±´ì˜ ì¡°í•©ìœ¼ë¡œ ì •ì˜ë¨\n",
    "\n",
    "<img src=\"img/3_PPO_2.png\" width=\"500\">\n",
    "\n",
    "- Frozen Modelê³¼ Non-frozen(trainable) Modelì˜ í…ìŠ¤íŠ¸ ì¶œë ¥ í™•ë¥ ê°„ KL divergenceë¥¼ ê³„ì‚°\n",
    "- trainable Modelì˜ weightê°€ ì™„ì „íˆ ë°”ë€ŒëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  Reward Modelì— ë§ë„ ë˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥ì„ ì‹œì‘í•˜ëŠ” ê²ƒì„ ë°©ì§€\n",
    "\n",
    "\n",
    "<img src=\"img/3_PPO_3.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- PPO process\n",
    "[1] ì´ˆê¸°í™”ë¥¼ ìœ„í•´ intial probs(initial output text probabilities)ë¥¼ new probs(new output text probabilities)ì™€ ë™ì¼í•˜ê²Œ ë§Œë“¬\n",
    "\n",
    "- while:\n",
    "    - [2] New probsì™€ initial probsê°„ ratioì„ ê³„ì‚°í•¨\n",
    "    - [3] ì•„ë˜ ê³µì‹ì— ë”°ë¼ lossë¥¼ ê³„ì‚°í•¨.\n",
    "        - loss = -min(ratio * R, clip(ratio, 0.8, 1.2) * R)\n",
    "            - R = reward + KL (or 0.8*reward + 0.2*KLì™€ ê°™ì€ weighted average)\n",
    "            - clip(ratio, 0.8, 1.2) â†’ 0.8 â‰¤ ratio â‰¤ 1.2\n",
    "    - [4] Lossë¥¼ backpropagatingí•˜ì—¬ SFT Modelì˜ weightë¥¼ ì—…ë°ì´íŠ¸í•¨\n",
    "\n",
    "    - [5] ìƒˆë¡­ê²Œ ì—…ë°ì´íŠ¸ëœ SFT ëª¨ë¸ë¡œ new probsë¥¼ ê³„ì‚°í•¨\n",
    "\n",
    "    - [6] 2ë²ˆë¶€í„° 6ë²ˆì„ N ë²ˆ ë°˜ë³µí•¨\n",
    "\n",
    "\n",
    "\n",
    "- [loss1](https://github.com/hpcaitech/ColossalAI/blob/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/experience_maker/naive.py#L7)\n",
    "- [loss2](https://github.com/hpcaitech/ColossalAI/blob/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/models/utils.py#L31)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "-fo6KWWcpdxh"
   },
   "outputs": [],
   "source": [
    "# ## setup(1min)\n",
    "# # torch ë²„ì „ ë‹¤ìš´. torch>=2.0 ì—ì„  colosalaiê°€ ë™ì‘ì•ˆí•¨\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"Torch version:{}\".format(torch.__version__))\n",
    "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "# # for ColossalAI\n",
    "# !pip install colossalai==0.2.7\n",
    "\n",
    "# # setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "E3Ytmwj-diQ6"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.models.opt import OPTActor, OPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "\n",
    "## wy ì¶”ê°€\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## clossalAI error í•´ê²°\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '2'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '42043'\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Ctd5oDVCdiJJ",
    "outputId": "f420a335-886d-4eb3-f549-2adddd4913ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path_3_PPO='./data_kochatgpt/kochatgpt_3_PPO.jsonl', output_dir='./output_3_PPO', strategy='naive', model='gpt2', pretrain='skt/kogpt2-base-v2', num_episodes=1, max_timesteps=3, update_timesteps=3, max_epochs=5, train_batch_size=8, lora_rank=0, max_length=250, pretrain_actor='./output_1_SFT', pretrain_critic='./output_2_RM')\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_3_PPO', type=str, default='./data_kochatgpt/kochatgpt_3_PPO.jsonl')\n",
    "parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--num_episodes', type=int, default=10)\n",
    "parser.add_argument('--max_timesteps', type=int, default=3)\n",
    "parser.add_argument('--update_timesteps', type=int, default=3)\n",
    "parser.add_argument('--max_epochs', type=int, default=5)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.output_dir = './output_3_PPO'\n",
    "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "\n",
    "## ì´ê³³ ìˆ˜ì •!!\n",
    "args.pretrain_actor = './output_1_SFT'  # SFT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "args.pretrain_critic = './output_2_RM'  # RM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
    "# args.pretrain_actor = args.pretrain\n",
    "# args.pretrain_critic = args.pretrain\n",
    "\n",
    "args.num_episodes = 1\n",
    "args.max_epochs   = 5\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "E5YEhGiIdiBQ"
   },
   "outputs": [],
   "source": [
    "# configure strategy\n",
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "2ZJ4oN9BdnL6",
    "outputId": "2d0ed508-fa6b-471c-c329-a586ddf1621f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    if args.model == 'gpt2':\n",
    "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )    \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "#     elif args.model == 'bloom':\n",
    "#         actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
    "#         tokenizer.pad_token = tokenizer.eos_token            \n",
    "#     elif args.model == 'opt':\n",
    "#         actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "3OnJ76tgdnAr"
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "if args.strategy.startswith('colossalai'):\n",
    "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
    "else:\n",
    "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "3CPTGw5TeW9p"
   },
   "outputs": [],
   "source": [
    "# setting the models\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "jM3Tm4Lhdh5U",
    "outputId": "131194ca-a868-41c2-9439-39838990d22f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "# lsw add\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(list_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(tokenize_fn('I want you to act as a linux terminal.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(51200, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTCritic(\n",
       "  (model): GPT2Model(\n",
       "    (wte): Embedding(51201, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (model): GPT2Model(\n",
       "    (wte): Embedding(51201, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model.training, actor.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140217472857472, 140217473349376, 140217472857856, 140217483856960)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(initial_model), id(reward_model), id(actor), id(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "DAS01J_iedt5",
    "outputId": "395b8d38-0950-4ca0-c6be-bd46747cd2b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/1]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:01<00:30, 30.12s/it]\n",
      "Train epoch [1/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0323]\u001b[A\n",
      "Train epoch [1/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.60it/s, actor_loss=0, critic_loss=0.0323]\u001b[A\n",
      "Train epoch [1/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.60it/s, actor_loss=0, critic_loss=1.39]  \u001b[A\n",
      "Train epoch [1/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.60it/s, actor_loss=0, critic_loss=1.39]\u001b[A\n",
      "Train epoch [1/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.60it/s, actor_loss=0, critic_loss=0.0949]\u001b[A\n",
      "Train epoch [1/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.60it/s, actor_loss=0, critic_loss=0.0949]\u001b[A\n",
      "\n",
      "Train epoch [2/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.223]\u001b[A\n",
      "Train epoch [2/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.62it/s, actor_loss=0, critic_loss=0.223]\u001b[A\n",
      "Train epoch [2/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.62it/s, actor_loss=0, critic_loss=0.646]\u001b[A\n",
      "Train epoch [2/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.646]\u001b[A\n",
      "Train epoch [2/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.679]\u001b[A\n",
      "Train epoch [2/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.679]\u001b[A\n",
      "\n",
      "Train epoch [3/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0821]\u001b[A\n",
      "Train epoch [3/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.62it/s, actor_loss=0, critic_loss=0.0821]\u001b[A\n",
      "Train epoch [3/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.62it/s, actor_loss=0, critic_loss=0.0293]\u001b[A\n",
      "Train epoch [3/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.62it/s, actor_loss=0, critic_loss=0.0293]\u001b[A\n",
      "Train epoch [3/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.62it/s, actor_loss=0, critic_loss=0.0679]\u001b[A\n",
      "Train epoch [3/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.0679]\u001b[A\n",
      "\n",
      "Train epoch [4/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [4/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.299]\u001b[A\n",
      "Train epoch [4/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.61it/s, actor_loss=0, critic_loss=0.299]\u001b[A\n",
      "Train epoch [4/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.61it/s, actor_loss=0, critic_loss=0.342]\u001b[A\n",
      "Train epoch [4/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.342]\u001b[A\n",
      "Train epoch [4/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.238]\u001b[A\n",
      "Train epoch [4/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.238]\u001b[A\n",
      "\n",
      "Train epoch [5/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [5/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0715]\u001b[A\n",
      "Train epoch [5/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.61it/s, actor_loss=0, critic_loss=0.0715]\u001b[A\n",
      "Train epoch [5/5]:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:01,  1.61it/s, actor_loss=0, critic_loss=0.0408]\u001b[A\n",
      "Train epoch [5/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.0408]\u001b[A\n",
      "Train epoch [5/5]:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.107] \u001b[A\n",
      "Train epoch [5/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.107]\u001b[A\n",
      "Episode [1/1]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:38<00:00, 32.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# configure trainer\n",
    "trainer = PPOTrainer(strategy,\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=args.max_epochs,\n",
    "                     train_batch_size=args.train_batch_size,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=512,\n",
    "#                      max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "## train!\n",
    "trainer.fit(list_prompt,  # ì…ë ¥ prompt\n",
    "            num_episodes=args.num_episodes,\n",
    "            max_timesteps=args.max_timesteps,\n",
    "            update_timesteps=args.update_timesteps)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(actor_optim,\n",
    "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '</s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '</s>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "mjmSYfy2xBi5",
    "outputId": "7d267181-a921-4bdd-d2d7-7cdfbda4767a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ì´ê¸° ë•Œë¬¸ì—, ë²„ë¦¬ëŠ” ê³ ê¸°ìš© ìœ¡ìˆ˜ì— ëŒ€í•œ ì •ë³´ëŠ” ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ìœ¡ì†ŒëŠ” ë§¤ìš´ë§›ì„ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•˜ëŠ” ìŒì‹ê±°ë¦¬ ì¤‘ í•˜ë‚˜ì´ë©°, ë³´í†µ ë¶€ë“œëŸ¬ìš´ ì†ŒìŠ¤ì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë§›ê³¼ í–¥ìœ¼ë¡œ ì¸í•œ ë¶ˆë ˆê¸° ë¬¸ì œë¡œ ë¶ˆê³ ê¸°ê°€ ì§ˆì²™í•˜ëŠ” ìƒí™©ì´ ë°œìƒí• ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë§›ê³¼ ê±´ê°• ìƒíƒœì— ëŒ€í•œ ì§€ì†ì ì¸ ê´€ë¦¬ì™€ í•¨ê»˜ ìƒíƒœì— ëŒ€í•´ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì§ì¥ì¸ì„ ëŒ€ìƒìœ¼ë¡œ ISAê³„ì¢Œë¥¼ íŒë§¤í•˜ê¸° ìœ„í•œ ê´‘ê³  ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜.\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì œê°€ëŠ” ì¸ê³µì§€ëŠ¥ ì–¸ì–´ëª¨ë¸ë¡œì¨ ì¸ê°„ì˜ ìê²©ì´ë‚˜ ì§€ìœ„ë§Œì„ ê°€ì§€ê³  ISAê°€ì…ìë¥¼ ì§ì ‘ í–‰ìœ„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ì˜ëª»ëœ ì§ˆë¬¸ì¸ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” ëª¨ë“  ê´€ê³„ì—ì„œ ì˜ ì•Œê³  ìˆìœ¼ë©°, ê°€ëŠ¥í•œ ë§ì€ ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤. ISAê°€ì… ê´€ë ¨í•˜ì—¬ ì—¬ëŸ¬ ê°€ì§€ ë¬¼ê±´ì— ëŒ€í•œ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´, ISAê°€ì… ìº í˜ì¸ì„ í†µí•´ ê°€ì…ì ì‹ ì›ì´ë‚˜ í™˜ê²½ì„ í‰ê°€í•˜ê³ , ê°€ì…ì ìê²©ì„ ì¦ëª…í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ëª»ëœ ì •ë³´ëŠ” ì ê·¹ì ìœ¼ë¡œ ì‚­ì œí•˜ì‹œê³ , ê±´ê°•í•œ ê°€ì…ì„ ìœ„í•´ ì¶©ë¶„í•œ ì •ë³´ë‚˜ í™œë™ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "í•œêµ­ì—ì„œ ì œì¼ ìœ ëª…í•œ ê°€ìˆ˜ì™€ ê·¸ ëŒ€í‘œê³¡ì„ ì•Œë ¤ì¤˜.\n",
      "\n",
      "### Response(ì‘ë‹µ):'CDM (Chemistry Delta)\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë†ì‹¬ì—ì„œ ì¶œì‹œí•œ ë¼ë©´ 5ê°€ì§€ì™€ ê·¸ íŠ¹ì§•ì„ ì•Œë ¤ì¤˜\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë†ì‹¬ì—ì„œ ì¶œì‹œí•œ ë¼ë©´ì€ ì—¬ëŸ¬ê°€ì§€ ìš”ì†Œ, í˜•íƒœ, ì œì¡° ë°©ë²• ë“±ì´ ìˆìŠµë‹ˆë‹¤. ê° ì¢…ë¥˜ëŠ” ë°€ê°€ë£¨, ì†ŒìŠ¤, ì†ŒìŠ¤, ì†ŒìŠ¤, êµ­ë¬¼ ë“±ì´ íŠ¹ì§•ì…ë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ ë©´ì„ ë¿Œë ¤ ë§Œë“  ë§›ìˆëŠ” ë¼ë©´, ìœ¡ì „ ë°˜ì£½ì„ ì‚¬ìš©í•œ ì•ˆë„ë¼ ê³ êµ¬ë§ˆ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì—¬ëŸ¬ ê°€ì§€ ì‚¬ì´ë“œ ë©”ë‰´ë„ ìˆìŠµë‹ˆë‹¤.  \\n \\n- ì ìš©ëœ ì¹˜í‚¨ ì¢…ë¥˜ì— ë”°ë¼ ë¼ì§€ê³ ê¸° ë“±ì‹¬ì´ ì°¸ê¹¨,í† ë°”ì´ì›”ë“œ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë¼ë©´ì„ ì œê³µí•©ë‹ˆë‹¤. \\n- ì¡°ë¦¬ë°©ë²•>\\n- ìœ¡ì „ì—ì„œ ìµœì´ˆë¡œ ê³ ì•ˆí•œ í‰ì–´ë¡œ ë§Œë“¤ì–´ì§„, ê³ ê¸°ë‚˜ ì–‘íŒŒ, ì¬ê·¤ ë“±ì´ ëŒ€í‘œì ì…ë‹ˆë‹¤. \\n- ì œí’ˆ ì´ë¦„ìœ¼ë¡œëŠ” ë¼ì§€ ëª©ì‚´ë¼,ê°œì™€ ê³„ì•½ì·¨ ì´ë ‡ê²Œ ì—¬ëŸ¬ê°€ì§€ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# lsw added\n",
    "# eos token ì´í›„ ë¬¸ì¥ ìƒì„± ë°©ì§€ìš© \n",
    "actor.pad_token_id = tokenizer.pad_token_id\n",
    "actor.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args.max_length,\n",
    "                             do_sample=True,\n",
    "#                              top_k=50,\n",
    "#                              top_p=0.95,\n",
    "                             temprature = 0.8,\n",
    "                             no_repeat_ngram_size=6,\n",
    "                             pad_token_id = tokenizer.pad_token_id,\n",
    "                             eos_token_id = tokenizer.eos_token_id,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?', \n",
    "#     'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?', \n",
    "#     'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "#     'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?'\n",
    "    'ì§ì¥ì¸ì„ ëŒ€ìƒìœ¼ë¡œ ISAê³„ì¢Œë¥¼ íŒë§¤í•˜ê¸° ìœ„í•œ ê´‘ê³  ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜.',\n",
    "    'í•œêµ­ì—ì„œ ì œì¼ ìœ ëª…í•œ ê°€ìˆ˜ì™€ ê·¸ ëŒ€í‘œê³¡ì„ ì•Œë ¤ì¤˜.',\n",
    "    'ë†ì‹¬ì—ì„œ ì¶œì‹œí•œ ë¼ë©´ 5ê°€ì§€ì™€ ê·¸ íŠ¹ì§•ì„ ì•Œë ¤ì¤˜'\n",
    "]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTPxdCO0A6yt"
   },
   "source": [
    "#### ì¢‹ì€ê¸€ ìƒì„±ê¸°ì™€ ì¢‹ì€ê¸€ ì±„ì ê¸°ë¡œ ê°•í™”í•™ìŠµì„ í•˜ì—¬ ChatGPT-replicaë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.  \n",
    "#### ``output_3_PPO`` í´ë”ì— í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
    "#### ë~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NY84CFCvA6N-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hvp1QP0VA582"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "ihMgjx60hCHM"
   },
   "source": [
    "### inference PPO actor\n",
    "- [ref](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "PcRr_cZThBqa"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from chatgpt.models.bloom import BLOOMActor\n",
    "from chatgpt.models.gpt import GPTActor\n",
    "from chatgpt.models.opt import OPTActor\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\":\n",
    "    (\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "     \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì™€ ì¶”ê°€ì  ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ì„ ì´ë£¨ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\nìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "     \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n### Response(ì‘ë‹µ):\"\n",
    "     ),\n",
    "    \"prompt_no_input\":\n",
    "    (\"Below is an instruction that describes a task.\\n\"\n",
    "     \"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\nëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\\n\\n\"\n",
    "     \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "o_hA3_OChEwB",
    "outputId": "380df41b-915b-435b-e073-f438c167e8d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model',\n",
    "                    default='gpt2',\n",
    "                    choices=['gpt2', 'bloom', 'opt'])\n",
    "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--model_path', type=str, default=None)\n",
    "parser.add_argument('--input',\n",
    "                    type=str,\n",
    "                    default='Question: How are you ? Answer:')\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args_inference = parser.parse_args([])\n",
    "\n",
    "args_inference.model = 'gpt2'\n",
    "args_inference.pretrain = 'skt/kogpt2-base-v2'\n",
    "args_inference.model_directory = './output_3_PPO'\n",
    "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
    "\n",
    "# configure model, tokenizer\n",
    "if args_inference.model == 'gpt2':\n",
    "    actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
    "                                              padding_side=\"right\",\n",
    "                                              model_max_length=512)\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    })\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "elif args_inference.model == 'bloom':\n",
    "    actor = BLOOMActor(pretrained=args_inference.pretrain).to(\n",
    "        torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "elif args_inference.model == 'opt':\n",
    "    actor = OPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported model \"{args_inference.model}\"')\n",
    "\n",
    "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
    "actor.model.load_state_dict(state_dict);\n",
    "\n",
    "actor.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
      "\n",
      "### Response(ì‘ë‹µ):'ìŒì‹ë¬¼ ì£¼ë¬¸ ì‹œ í•´ë‹¹ ìŒì‹ì ì˜ ë©”ë‰´ì™€ ì‚¬ì´ì¦ˆë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì£¼ë¬¸í•˜ì‹œëŠ” ì‹ë‹¹ì— ë¬¸ì˜í•˜ì‹œê±°ë‚˜, ì˜¨ë¼ì¸ ë§ˆì¼“í”Œë ˆì´ìŠ¤ì—ì„œ ì§ì ‘ ì£¼ë¬¸í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ì§ì¥ì¸ì„ ëŒ€ìƒìœ¼ë¡œ ISAê³„ì¢Œë¥¼ íŒë§¤í•˜ê¸° ìœ„í•œ ê´‘ê³  ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜.\n",
      "\n",
      "### Response(ì‘ë‹µ):'ISAê³„ì¢Œ íŒë§¤ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´ì„œëŠ” í•´ë‹¹ ì¡°ì§ì˜ ì§ì›ì´ë‚˜ ê¸°ìì˜ ë©´ë°€í•œ ê²€í†  ë“±ì„ í†µí•œ ì •ë³´ ìˆ˜ì§‘ì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹ íšŒì‚¬ì˜ ì§ì›ì´ ì‹¤ì œë¡œ ISAê³„ì¢Œ íŒë§¤ë¥¼ ë§¡ê³  ìˆë‹¤ë©´, ê·¸ ì´í›„ì— ê°€ì…í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤. ê¸°ìˆ™ì‚¬ë‚˜ í•™êµ ë“±ì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ í™œìš©í•œ ì•ˆë‚´ë¥¼ ì œê³µí•˜ê±°ë‚˜, ì§ì› í†µê´€ë¦¬, ê³„ì•½ ì¢…ë£Œ ë“± í›„ì† ì¡°ì¹˜ë¥¼ ì‹œí–‰í•©ë‹ˆë‹¤. ë˜í•œ í•´ë‹¹ ê¸°ê´€ì˜ í™ˆí˜ì´ì§€ë¥¼ í†µí•´ ISA ê³„ì¢Œ íŒë§¤ë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "í•œêµ­ì—ì„œ ì œì¼ ìœ ëª…í•œ ê°€ìˆ˜ì™€ ê·¸ ëŒ€í‘œê³¡ì„ ì•Œë ¤ì¤˜.\n",
      "\n",
      "### Response(ì‘ë‹µ):'ì¢‹ìŠµë‹ˆë‹¤. í•œêµ­ ê°€ìˆ˜ê°€ ëª‡ ê°œ ìˆ˜ë¡ë˜ì–´ ìˆëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ëª…ê³¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\\n\\n1. íŒë®¤ì§€ì…˜ Lot I Would\\n1. ìœ¤ìƒ‰ê°€ìˆ˜ lot sugar\\n2. ì½”ë¦¬ì•ˆì»¬ë ‰ì…˜ Korean Infoyd\\n3. RMPSKë¬¼ê²° U\\n4. MC ëª½í‚¤ Fare\\n5. DVD ê±¸ì‘ ì‹œë¦¬ì¦ˆs of chambine\\n6. ë¸Œë£¨ë§ˆì‰¬kissang Branch\\n7. í‚¹ ì˜¤ë¸Œ ë°”ë¹Œë¼\\n9. ì‚°ì´ì—¬å‘¨IIT Wedshow\\n10. Wind MON 9. CHP White Luk\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "ëª…ë ¹ì–´ì— ë”°ë¥¸ ìš”ì²­ì„ ì ì ˆíˆ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "\n",
      "### Instruction(ëª…ë ¹ì–´):\n",
      "ë†ì‹¬ì—ì„œ ì¶œì‹œí•œ ë¼ë©´ 5ê°€ì§€ì™€ ê·¸ íŠ¹ì§•ì„ ì•Œë ¤ì¤˜\n",
      "\n",
      "### Response(ì‘ë‹µ):'ë†ì‹¬ì˜ ë¼ë©´ 5ëŒ€ì¹™(1) ì›í™œí•œ ìœ í†µ: ì›í™œí•œ ì‹ìŠµê´€, ê±´ê°•í•œ ì‹ìŠµê´€, ì†Œí†µê´€ë¦¬ ì˜í•˜ê³  ì¶©ë¶„í•œ ìˆ˜ë©´ ìœ ì§€, ê±´ê°•í•œ ì‹ìŠµê´€ ë“±)\\n\\n2) ì²­ê²°í•œ ì‹ì¬ë£Œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ì„ í•œ ë¼ë©´ì„ ë§Œë“¤ì–´ ì‚¬ìš©\\n\\n3) ì§€ì†ì ì¸ ê±´ê°• ìœ ì§€: ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì„ ìœ ì§€í•˜ê³ , ìˆ˜ë¶„ ê³µê¸‰ì„ ì˜ ì§€í‚¤ë©°, ì˜ì–‘ ê· í˜• ìˆëŠ” ì‹ìŠµê´€ì„ ìœ ì§€í•œë‹¤.\\n\\n5) ë§›ìˆê²Œ ë¨¹ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©° ê±´ê°•í•œ ì‹ìŠµê´€ì„ ìœ ì§€í•˜ë©°, ê±´ê°•í•œ ì‹ìŠµê´€ì„ ìœ ì§€í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ë‹¤.\\n\\nìœ„ì™€ ê°™ì´ ë¼ë©´ì„ ë¨¹ì„ ë•ŒëŠ” ì‹ìŠµê´€ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•œí•˜ë©°, ì‹í’ˆ ìì²´ê°€ ê±´ê°•ì— ì¢‹ì§€ ì•Šì€ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# lsw added\n",
    "# eos token ì´í›„ ë¬¸ì¥ ìƒì„± ë°©ì§€ìš© \n",
    "actor.pad_token_id = tokenizer.pad_token_id\n",
    "actor.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args_inference.max_length,\n",
    "                             do_sample=True,\n",
    "#                              top_k=50,\n",
    "#                              top_p=0.95,\n",
    "                             temprature = 0.8,\n",
    "                             no_repeat_ngram_size=6,\n",
    "                             pad_token_id = tokenizer.pad_token_id,\n",
    "                             eos_token_id = tokenizer.eos_token_id,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?', \n",
    "#     'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?', \n",
    "#     'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
    "#     'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?'\n",
    "    'ì§ì¥ì¸ì„ ëŒ€ìƒìœ¼ë¡œ ISAê³„ì¢Œë¥¼ íŒë§¤í•˜ê¸° ìœ„í•œ ê´‘ê³  ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì¤˜.',\n",
    "    'í•œêµ­ì—ì„œ ì œì¼ ìœ ëª…í•œ ê°€ìˆ˜ì™€ ê·¸ ëŒ€í‘œê³¡ì„ ì•Œë ¤ì¤˜.',\n",
    "    'ë†ì‹¬ì—ì„œ ì¶œì‹œí•œ ë¼ë©´ 5ê°€ì§€ì™€ ê·¸ íŠ¹ì§•ì„ ì•Œë ¤ì¤˜'\n",
    "]\n",
    "\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "G205eduIAyTw"
   },
   "source": [
    "#### ì¢‹ì€ê¸€ ìƒì„±ê¸°ì™€ ì¢‹ì€ê¸€ ì±„ì ê¸°ë¡œ ê°•í™”í•™ìŠµì„ í•˜ì—¬ ChatGPT-replicaë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.  \n",
    "#### ``output_3_PPO`` í´ë”ì— í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
    "#### ë~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true,
    "id": "Cx52ECfOPalV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_YCMEVRj3rp"
   },
   "source": [
    "# END!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "vHgdUwoJPalV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Wp82sZANSMVa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ecd2ec82a804860a78885c97fc250b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13035ae8dc2741a8bd0a96c234300af2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c2de103392e48ba954c8df37612b395": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_975c6155ea2c476684ea2b6df5483e1b",
      "max": 2825034,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c20354dbe20844c28fe1455c765db2f6",
      "value": 2825034
     }
    },
    "1d0e339ffba14e8d91368770385f1db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9578bba68cfa4c03a21d6251c20053d5",
       "IPY_MODEL_36d0515d2c2847868b3d9dd2ea05f25a",
       "IPY_MODEL_bbd8f769a05048bca99d1075bfe47b6e"
      ],
      "layout": "IPY_MODEL_fa47d352268a4be388b820f9a4a6c5a5"
     }
    },
    "2e3e77f407bc48279570721105a621ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3532edab6de845b589a83f395917578f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e1646bc324c40428c9fe9b7ab8a6921",
       "IPY_MODEL_1c2de103392e48ba954c8df37612b395",
       "IPY_MODEL_56fca42756f94eabaf830bc86d18c4a7"
      ],
      "layout": "IPY_MODEL_4cb558fcf41c4406b15c98af203bf272"
     }
    },
    "36d0515d2c2847868b3d9dd2ea05f25a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f541cd5c2c704129855a39d1c81a48b3",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7b37f9f9d764172828cb005d2b9f633",
      "value": 1000
     }
    },
    "3c309d97eb5d43c798d6db8f49a8e434": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cb558fcf41c4406b15c98af203bf272": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56fca42756f94eabaf830bc86d18c4a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13035ae8dc2741a8bd0a96c234300af2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3c309d97eb5d43c798d6db8f49a8e434",
      "value": " 2.83M/2.83M [00:00&lt;00:00, 13.4MB/s]"
     }
    },
    "59d060b9aa65450c8828de299b2b3498": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_674175399071436b8da7cd07ebaa29c8",
      "max": 513302779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c03b82f323e45dbaab32148f00a6938",
      "value": 513302779
     }
    },
    "674175399071436b8da7cd07ebaa29c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7180bc85cb0b4c20b65365c5e84767d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78ff1615fa4b4d8ab51c4683a43f4182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a12232f55c5447fb0284ca001cc0023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "930bd459e2794a32ba4c303b31aaf159": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7180bc85cb0b4c20b65365c5e84767d4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_93479918dcbd40df9a733045e116a90b",
      "value": " 513M/513M [00:05&lt;00:00, 86.0MB/s]"
     }
    },
    "93479918dcbd40df9a733045e116a90b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9578bba68cfa4c03a21d6251c20053d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ecd2ec82a804860a78885c97fc250b2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c677b05ae99841c6879009aa20f2e5f2",
      "value": "Downloading (â€¦)lve/main/config.json: 100%"
     }
    },
    "95adfba856d54c50a4e1d71b8b4a3308": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "975c6155ea2c476684ea2b6df5483e1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c03b82f323e45dbaab32148f00a6938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e1646bc324c40428c9fe9b7ab8a6921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95adfba856d54c50a4e1d71b8b4a3308",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2e3e77f407bc48279570721105a621ed",
      "value": "Downloading (â€¦)/main/tokenizer.json: 100%"
     }
    },
    "a6fc9ca6dd874b3b80fe9aa732ec5d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cab874fdbbc0434f876661f578f3f855",
       "IPY_MODEL_59d060b9aa65450c8828de299b2b3498",
       "IPY_MODEL_930bd459e2794a32ba4c303b31aaf159"
      ],
      "layout": "IPY_MODEL_b887240c83db4b77b8bb8391ba0364b4"
     }
    },
    "b7b37f9f9d764172828cb005d2b9f633": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b887240c83db4b77b8bb8391ba0364b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbd8f769a05048bca99d1075bfe47b6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d50332475bbe429f9ef61ba26609cdc6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_78ff1615fa4b4d8ab51c4683a43f4182",
      "value": " 1.00k/1.00k [00:00&lt;00:00, 31.8kB/s]"
     }
    },
    "c20354dbe20844c28fe1455c765db2f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c677b05ae99841c6879009aa20f2e5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cab874fdbbc0434f876661f578f3f855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daf3185604a54f268a5b5f3a456c0f01",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7a12232f55c5447fb0284ca001cc0023",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "d50332475bbe429f9ef61ba26609cdc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf3185604a54f268a5b5f3a456c0f01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f541cd5c2c704129855a39d1c81a48b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa47d352268a4be388b820f9a4a6c5a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
