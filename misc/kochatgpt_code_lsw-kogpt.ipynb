{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLUYAx-jSMVQ"
   },
   "source": [
    "# Make ChatGPT-replica!!\n",
    "\n",
    "230421, by wygo\n",
    "\n",
    "#### 230421\n",
    "- GPU 설정 후 모두실행 하면 STEP1~3까지 모두 돌아갈수있게 수정\n",
    "- torch 1.x대 버전으로 다운\n",
    "- RM모델 점수확인코드 추가\n",
    "\n",
    "#### 230320\n",
    "- ChatGPT는 공개 코드가 없습니다.\n",
    "- 본 세미나에서는 ChatGPT를 만든 원리인 GPT fine-tuning, 강화학습(PPO), RLHF, ChatGPT 데이터셋 구축에 대해 다루고 코드 실습을 합니다.\n",
    "- 만들어진 모델을 활용만 하는 건 재미없잖아요??\n",
    "- 우리 분야만의 ChatGPT(한국어/전문분야)를 직접 만드는 방법을 소개합니다.\n",
    "    - ※ 구현 모델은 ChatGPT-replica입니다. 실제 ChatGPT와 다를 수 있습니다.\n",
    "    - ※ GPT3가 아닌 GPT2+RLHF로 구현합니다. 거대언어모델로 개발시 어려움이 있을 수 있습니다.\n",
    "    - ※ 실습환경: Jupyter or Colab, 선수 지식: 파이썬\n",
    "\n",
    "\n",
    "### Reference\n",
    "- [code_TRL](https://github.com/lvwerra/trl)\n",
    "- [code_Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n",
    "- [code_Alpaca_lora](https://github.com/tloen/alpaca-lora)\n",
    "\n",
    "- [blog_colossal-ai-chatgpt](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt)\n",
    "- [blog_RLHF_huggingface](https://huggingface.co/blog/rlhf)\n",
    "\n",
    "\n",
    "### ChatGPT 학습 방법\n",
    "- STEP 1) Prompt 라이브러리에서 샘플링하고 사람의 응답을 수집한 데이터를 사용하여 사전 학습된 대규모 언어 모델을 fine-tuning\n",
    "- STEP 2) Prompt 라이브러리에서 샘플링하고, 대규모 언어 모델을 사용하여 여러 응답을 생성, 이러한 응답의 순위를 수동으로 지정하고, 인간의 선호도에 맞게 보상 모델(Reward Model)을 학습\n",
    "- STEP 3) 1단계의 지도 미세 조정 모델과 2단계의 보상 모델을 기반으로 강화 학습 알고리즘을 사용하여 대규모 언어 모델을 추가로 훈련합니다.\n",
    "\n",
    "###  ChatGPT 개발 Requirement\n",
    "- 데이터(RLHF)\n",
    "- LLM 모델(GPT3급)\n",
    "- GPU\n",
    "    - 수천 GB의 GPU 메모리가 필요\n",
    "    - 일반적인 데이터 병렬 기술로도 X\n",
    "    - 최소 64개의 80GB A100 GPU가 필요\n",
    "\n",
    "###  ChatGPT-replica 실습 Requirement\n",
    "- 데이터(RLHF): data_kochatgpt\n",
    "- LLM 모델: GPT2\n",
    "- GPU: Colab\n",
    "\n",
    "### ChatGPT-replica 모델 정리\n",
    "- [Step1_fine tuning code](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
    "- [huggingface_TRL](https://github.com/lvwerra/trl)\n",
    "    - RL을 구현하기 위한 코드 제공\n",
    "    - ChatGPT를 위해 코드수정이 많이많이 필요함\n",
    "- [LLaMA](https://github.com/facebookresearch/llama)\n",
    "    - ChatGPT보다 모델 크기가 작으면서도 성능이 좋은 모델 공개\n",
    "    - 한국어..추가학습..\n",
    "- [ChatLLaMA](https://github.com/juncongmoo/chatllama)\n",
    "    - LLaMA를 Chat 형식으로 학습하도록 강화학습 코드 제공\n",
    "    - GPT3기반 [대화 데이터셋 구축 코드](https://github.com/juncongmoo/chatllama/blob/main/generate_dataset.py) 제공, 수정 많이 필요 \n",
    "- [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n",
    "    - Instruction 데이터 생성 및 SFT만\n",
    "\n",
    "- [KoAlpaca](https://github.com/Beomi/KoAlpaca)\n",
    "    - 한국어 Instruction 데이터 생성 및 SFT만\n",
    "\n",
    "- [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)\n",
    "    - Low-Rank LLaMA Instruct-Tuning, SFT만\n",
    "\n",
    "    \n",
    "- **[ColossalAI](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ChatGPT)**\n",
    "    - step2 RM 학습과 step3 PPO 코드 깔끔하게 제공\n",
    "    - Multi-GPU로 DDP, ColossalAIStrategy, LoRA 학습코드 제공!!\n",
    "    \n",
    "- **ColossalAI 장점**\n",
    "    - ColossalAI는 pytorch에 비해 추론시 1.4배 빠르고, 학습시 7.7배 빠르다!!\n",
    "    - ColossalAI는 pytorch와 비교해 10.3배 큰 모델을 처리할수 있다!!\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png\" width=\"800\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "-_Cqx0AySMVS"
   },
   "source": [
    "### Step 0) Prepare RLHF dataset\n",
    "\n",
    "- 총 3step을 학습하기 위해 3가지 데이터셋이 필요합니다.\n",
    "- [데이터셋 예시](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama#dataset-preparation)\n",
    "- [예시 데이터셋 1](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
    "- [예시 데이터셋 2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
    "\n",
    "step1) SFT(actor_training_data): SFT 지도 미세 조정에 사용되는 JSON 데이터\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\",\n",
    "        \"completion\": \"\"        \n",
    "    }, ...\n",
    "]\n",
    "```\n",
    "\n",
    "step2) RM 모델 학습용 데이터셋(reward_training_data): 보상 모델 학습에 사용되는 JSON 데이터셋. 한 prompt에 대해 여러 완성된 문장이 있고, 이 문장들의 ranking을 사람이 매김\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\",\n",
    "        \"completion_1\": \"\",\n",
    "        \"completion_2\": \"\",\n",
    "        \"completion_3\": \"\",            \n",
    "        \"ranking\": [1, 0, 2]\n",
    "    }, ...\n",
    "]\n",
    "```\n",
    "    \n",
    "step3) PPO 학습 입력 데이터셋(rlhf_training_data): RLHF 훈련에 사용되는 JSON 데이터셋, 사용자 입력 prompt로만 구성\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\"\n",
    "    }, ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "7igAgV-5SMVT"
   },
   "source": [
    "### step0) Colab 환경 설정\n",
    "\n",
    "#### 설치(python>=3.8)\n",
    "```python\n",
    "## setup(1min)\n",
    "# torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__))\n",
    "print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!mv KoChatGPT/data_kochatgpt .\n",
    "!mv KoChatGPT/img .\n",
    "\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.13.1\n",
      "cuda version: 11.6\n",
      "cudnn version:8302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__))\n",
    "print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "LIg6pniqSMVU",
    "outputId": "1c57d04f-a2e3-4747-b76f-27d35750a28c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc17108f9d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/colossalai/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc17108e9e0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/colossalai/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting colossalai==0.2.7\n",
      "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (1.22.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (5.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (23.0)\n",
      "Collecting pre-commit\n",
      "  Downloading pre_commit-3.3.1-py2.py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich\n",
      "  Downloading rich-13.3.5-py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (8.1.3)\n",
      "Collecting fabric\n",
      "  Downloading fabric-3.0.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contexttimer\n",
      "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (1.11.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from colossalai==0.2.7) (1.13.1)\n",
      "Collecting invoke>=2.0\n",
      "  Downloading invoke-2.1.1-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting paramiko>=2.4\n",
      "  Downloading paramiko-3.1.0-py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nodeenv>=0.11.1\n",
      "  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n",
      "Collecting identify>=1.0.0\n",
      "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cfgv>=2.0.0\n",
      "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from pre-commit->colossalai==0.2.7) (6.0)\n",
      "Collecting virtualenv>=20.10.0\n",
      "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from torch->colossalai==0.2.7) (4.4.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (65.5.0)\n",
      "Collecting bcrypt>=3.2\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.2/593.2 kB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynacl>=1.5\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (38.0.1)\n",
      "Collecting filelock<4,>=3.11\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs<4,>=3.2\n",
      "  Downloading platformdirs-3.5.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
      "Building wheels for collected packages: colossalai, contexttimer\n",
      "  Building wheel for colossalai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896481 sha256=63622119541117851978680a9d7f1eb27e76976fe7069003ab9c5198d3804bdb\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/0c/72/9b13e70297cc84e934883264ca7ca4763d1ce867328d73d491\n",
      "  Building wheel for contexttimer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5805 sha256=4d8dbe3e11a23c3f46570dbffebd2891a043060129a44845f2fb936681e6aba1\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/11/19/7331f9a8b7c18c5a2c00a6a5d83d5767c831bdc1c0c0bce247\n",
      "Successfully built colossalai contexttimer\n",
      "Installing collected packages: distlib, contexttimer, pygments, platformdirs, nodeenv, mdurl, invoke, identify, filelock, cfgv, bcrypt, virtualenv, pynacl, markdown-it-py, rich, pre-commit, paramiko, fabric, colossalai\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.1.1\n",
      "    Uninstalling platformdirs-3.1.1:\n",
      "      Successfully uninstalled platformdirs-3.1.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.6.0\n",
      "    Uninstalling filelock-3.6.0:\n",
      "      Successfully uninstalled filelock-3.6.0\n",
      "Successfully installed bcrypt-4.0.1 cfgv-3.3.1 colossalai-0.2.7 contexttimer-0.3.3 distlib-0.3.6 fabric-3.0.1 filelock-3.12.0 identify-2.5.24 invoke-2.1.1 markdown-it-py-2.2.0 mdurl-0.1.2 nodeenv-1.7.0 paramiko-3.1.0 platformdirs-3.5.0 pre-commit-3.3.1 pygments-2.15.1 pynacl-1.5.0 rich-13.3.5 virtualenv-20.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m[Errno 2] No such file or directory: 'KoChatGPT/colossalai_ChatGPT_230319/'\n",
      "/workspace/KoChatGPT\n",
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m/\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.10/site-packages (0.27.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain==0.0.113\n",
      "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (1.10.7)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from langchain==0.0.113) (6.0)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (1.22.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (2.28.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.113) (3.8.4)\n",
      "Collecting SQLAlchemy<2,>=1\n",
      "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.4)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.113) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.7/613.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, mypy-extensions, marshmallow, greenlet, typing-inspect, SQLAlchemy, marshmallow-enum, dataclasses-json, langchain\n",
      "Successfully installed SQLAlchemy-1.4.48 dataclasses-json-0.5.7 greenlet-2.0.2 langchain-0.0.113 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 tenacity-8.2.2 typing-inspect-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ## setup(1min)\n",
    "# # torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"Torch version:{}\".format(torch.__version__))\n",
    "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd ./colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tyf1bUtSMVU"
   },
   "source": [
    "### Step 1) SFT: 질문에 대답을 잘하는 모델 만들기\n",
    "- [fine tuning code_1](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
    "- [fine tuning code_2](https://github.com/Beomi/KoAlpaca/blob/main/train.py)\n",
    "\n",
    "- SFT(Supervised Fine Tuning)\n",
    "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
    "- 기존 GPT3는 다음 단어를 잘 맞추는 모델. But 질문에 대해 답을 맞추는 모델이 X\n",
    "- 질문에 응답을 잘하도록 SFT 수행\n",
    "- 먼저 사람이 지시에 대한 대답을 직접 작성(데이터 13,000개)하고, 이 데이터셋으로 SFT\n",
    "- 데이터: 질문-응답 쌍 데이터셋(12,000개)\n",
    "- 예시)\n",
    "    - 질문(prompt): 인공지능을 설명해보세요\n",
    "    - 응답(completion): 인공지능은 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현하려는 컴퓨터 과학의 세부분야 중 하나이다. ...  \n",
    "\n",
    "\n",
    "- **SFT 예시**  \n",
    "<img src=\"img/1_SFT_1.png\" width=\"500\">  \n",
    "\n",
    "- **모델 입출력 예시**  \n",
    "<img src=\"img/image_step1.JPG\" width=\"500\">  \n",
    "\n",
    "- **전체 구조**  \n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png\" width=\"500\">\n",
    "\n",
    "- **데잍어셋 형태**\n",
    "step1) SFT(actor_training_data): SFT 지도 미세 조정에 사용되는 JSON 데이터\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"prompt\": \"\",\n",
    "        \"completion\": \"\"        \n",
    "    }, ...\n",
    "]\n",
    "```\n",
    "\n",
    "- **결과물**\n",
    "    - Before: 다음 단어만 잘 생성 했었음\n",
    "    - After: 질문에 ‘잘’ 대답하는 모델\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '/workspace/kogpt/kogpt-ft-3',\n",
    "    \"kakaobrain/kogpt\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "51S4uJpbSMVV"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4OzdlU9SMVV",
    "outputId": "a7c59a70-6051-40ea-837b-9a9fb32ff847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path_1_SFT='./data_kochatgpt/kochatgpt_1_SFT.jsonl', model_name=('/workspace/kogpt/kogpt-ft-3',), max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT-kogpt')\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_1_SFT', type=str, default='./data_kochatgpt/kochatgpt_1_SFT.jsonl')\n",
    "parser.add_argument('--model_name', type=str, default='kogpt')\n",
    "parser.add_argument('--max_epochs', type=int, default=2)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--output_dir', type=str, default='./output_1_SFT-kogpt')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.model_name = '/workspace/kogpt/kogpt-ft-3',\n",
    "\n",
    "args.max_epochs = 2\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498,
     "referenced_widgets": [
      "3532edab6de845b589a83f395917578f",
      "9e1646bc324c40428c9fe9b7ab8a6921",
      "1c2de103392e48ba954c8df37612b395",
      "56fca42756f94eabaf830bc86d18c4a7",
      "4cb558fcf41c4406b15c98af203bf272",
      "95adfba856d54c50a4e1d71b8b4a3308",
      "2e3e77f407bc48279570721105a621ed",
      "975c6155ea2c476684ea2b6df5483e1b",
      "c20354dbe20844c28fe1455c765db2f6",
      "13035ae8dc2741a8bd0a96c234300af2",
      "3c309d97eb5d43c798d6db8f49a8e434",
      "1d0e339ffba14e8d91368770385f1db9",
      "9578bba68cfa4c03a21d6251c20053d5",
      "36d0515d2c2847868b3d9dd2ea05f25a",
      "bbd8f769a05048bca99d1075bfe47b6e",
      "fa47d352268a4be388b820f9a4a6c5a5",
      "0ecd2ec82a804860a78885c97fc250b2",
      "c677b05ae99841c6879009aa20f2e5f2",
      "f541cd5c2c704129855a39d1c81a48b3",
      "b7b37f9f9d764172828cb005d2b9f633",
      "d50332475bbe429f9ef61ba26609cdc6",
      "78ff1615fa4b4d8ab51c4683a43f4182",
      "a6fc9ca6dd874b3b80fe9aa732ec5d69",
      "cab874fdbbc0434f876661f578f3f855",
      "59d060b9aa65450c8828de299b2b3498",
      "930bd459e2794a32ba4c303b31aaf159",
      "b887240c83db4b77b8bb8391ba0364b4",
      "daf3185604a54f268a5b5f3a456c0f01",
      "7a12232f55c5447fb0284ca001cc0023",
      "674175399071436b8da7cd07ebaa29c8",
      "9c03b82f323e45dbaab32148f00a6938",
      "7180bc85cb0b4c20b65365c5e84767d4",
      "93479918dcbd40df9a733045e116a90b"
     ]
    },
    "id": "2SrHOwC-SMVW",
    "outputId": "f0e5ab01-d6be-4b69-937a-7436faad91fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìķĪëħķ', 'íķĺ', 'ìĦ¸ìļĶ', '.', 'ĠíķľêµŃìĸ´', 'ĠGP', 'T', '-', '2', 'ĠìŀħëĭĪëĭ¤', '.', '😤', ':)', 'l^o']\n"
     ]
    }
   ],
   "source": [
    "## test & load skt gpt2 kroean\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"kakaobrain/kogpt\",\n",
    "        revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "print(tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\"))\n",
    "# ['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     './kogpt-ft-3',\n",
    "    \"kakaobrain/kogpt\",\n",
    "    revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    # torch_dtype='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498,
     "referenced_widgets": [
      "3532edab6de845b589a83f395917578f",
      "9e1646bc324c40428c9fe9b7ab8a6921",
      "1c2de103392e48ba954c8df37612b395",
      "56fca42756f94eabaf830bc86d18c4a7",
      "4cb558fcf41c4406b15c98af203bf272",
      "95adfba856d54c50a4e1d71b8b4a3308",
      "2e3e77f407bc48279570721105a621ed",
      "975c6155ea2c476684ea2b6df5483e1b",
      "c20354dbe20844c28fe1455c765db2f6",
      "13035ae8dc2741a8bd0a96c234300af2",
      "3c309d97eb5d43c798d6db8f49a8e434",
      "1d0e339ffba14e8d91368770385f1db9",
      "9578bba68cfa4c03a21d6251c20053d5",
      "36d0515d2c2847868b3d9dd2ea05f25a",
      "bbd8f769a05048bca99d1075bfe47b6e",
      "fa47d352268a4be388b820f9a4a6c5a5",
      "0ecd2ec82a804860a78885c97fc250b2",
      "c677b05ae99841c6879009aa20f2e5f2",
      "f541cd5c2c704129855a39d1c81a48b3",
      "b7b37f9f9d764172828cb005d2b9f633",
      "d50332475bbe429f9ef61ba26609cdc6",
      "78ff1615fa4b4d8ab51c4683a43f4182",
      "a6fc9ca6dd874b3b80fe9aa732ec5d69",
      "cab874fdbbc0434f876661f578f3f855",
      "59d060b9aa65450c8828de299b2b3498",
      "930bd459e2794a32ba4c303b31aaf159",
      "b887240c83db4b77b8bb8391ba0364b4",
      "daf3185604a54f268a5b5f3a456c0f01",
      "7a12232f55c5447fb0284ca001cc0023",
      "674175399071436b8da7cd07ebaa29c8",
      "9c03b82f323e45dbaab32148f00a6938",
      "7180bc85cb0b4c20b65365c5e84767d4",
      "93479918dcbd40df9a733045e116a90b"
     ]
    },
    "id": "2SrHOwC-SMVW",
    "outputId": "f0e5ab01-d6be-4b69-937a-7436faad91fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근육이 커지기 위해서는 근육에 지속적인 자극(stimulation)을 주어야 한다. 즉, 근력 운동은 근섬유를 손상시키고 그 후 회복되어 원상태로 돌아가는데 필요한 시간보다 더 많 은 기간 동안 반복적으로 수행해야만 효과가 있다\"라고 강조했습니다.. 또한 \"근력운동의 목적인 '최대산소섭취량' 증가와 같 이 중요하게 생각할 것들이 몇 가지있는데요~ 첫째! 유산소성 능력 향상 둘째!! 체지방 감소 셋째!!! 관절 건강 유지 넷째!!!! 유연성과 균형감각 증진 다섯째!!!!!!!! 심폐지구 력과 폐활량의 개선 여섯째????? 신체구성 변화입니다...^^ 위에서도 언급 했듯이 최대 산소 섭취량도 매우중요하지만 그것만큼이나 또 다른 요소들 도 고려해주셔야 합니다.... 그리고 마지막 여섯번째까지 가서 가장 크 게 신경써주시면 좋으실 부분인데요~~ 바로 체성분변화 입니다........체 성분이란 지방조직 + 단백질 조직 으로 구성된것인데 이것들의 비율 (% body fat mass / 체중kg당 부피 kg/m3 등 을 말합니다 ^^; 쉽죠??ᄏ ) 에 따라 몸매나 몸짱 여부등 여러가지 결과 를 볼 수 잇답니다 ᄒ 그렇다면 어떻게 해야\n"
     ]
    }
   ],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=256,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)\n",
    "\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "# generation_args = dict(\n",
    "#     num_beams=4,\n",
    "#     repetition_penalty=2.0,\n",
    "#     no_repeat_ngram_size=4,\n",
    "#     eos_token_id=375, # \\n\n",
    "#     max_new_tokens=64,\n",
    "#     do_sample=True,\n",
    "#     top_k=50,\n",
    "#     early_stopping=True\n",
    "# )\n",
    "# generator(\n",
    "#     [\"0 : **는 게임 좋아하니\\n1 :\",\n",
    "#     \"0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 :\",\n",
    "#     \"0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : \"],\n",
    "#     **generation_args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "NXT_zkuwSMVX"
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "# DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "# DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
    "        \"Write a detailed response that appropriately completes the request.\\n\\n\"\\\n",
    "        \"### Instruction:\\n{prompt}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\\n\"\n",
    "        \"Write a detailed response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_a_EbaW8SMVX",
    "outputId": "b41c3a6f-381c-4544-e041-3c29e7fcb28e"
   },
   "outputs": [],
   "source": [
    "# ## 모델 준비\n",
    "# model = AutoModelForCausalLM.from_pretrained(args.model_name).to('cuda')\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#     args.model_name,\n",
    "#     padding_side=\"right\",\n",
    "#     model_max_length=512,    \n",
    "# )\n",
    "# tokenizer.add_special_tokens(\n",
    "#     {\n",
    "#         \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "#         \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "#         \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "#     }\n",
    "# )    \n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     3,
     105
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E6u3j_YSMVX",
    "outputId": "4beba0f2-5a50-4d6e-8380-9c30217c7916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "## prepare data\n",
    "from typing import Optional, Dict, Sequence\n",
    "    \n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        \n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_input = 'input'  # 내 데이터엔 input이 없다\n",
    "        pattern_output = 'completion'  # output\n",
    "\n",
    "        ############################################################\n",
    "        ## load dataset\n",
    "        # 내 데이터셋엔 input이 없다\n",
    "#         data_path_1_SFT = 'data_kochatgpt/korean_chatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "        # {'prompt': '불고기용 고기 한우에요?',\n",
    "        #  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
    "        #  'tokens': 193}        \n",
    "\n",
    "        ############################################################\n",
    "        ## 데이터셋 만들기, source와 target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "\n",
    "        # 입력\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # 출력\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        ############################################################\n",
    "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
    "        \n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
    "        \n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
    "eval_dataset  = None  # eval은 안함\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "# check\n",
    "# print('input : %s'%train_dataset.input_ids[0])\n",
    "# print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "Uuh3PgkLSMVY",
    "outputId": "5509fe61-a95b-4942-f701-26327662fb85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">26</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>eval_dataset=eval_dataset,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>26 trainer.train()                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 # trainer.save_state()</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 # safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2717</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2714 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2715 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2716 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2718 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2719 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2720 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">488</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 488 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 491 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">197</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">194 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>197 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.64</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.05</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49.50</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.42</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated \n",
       "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m26\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0meval_dataset=eval_dataset,                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m26 trainer.train()                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m# trainer.save_state()\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m# safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1929\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1929 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2717\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2714 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2715 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2716 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2717 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2718 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2719 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2720 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m488\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 487 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 488 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 491 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m197\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m197 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m128.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m23.64\u001b[0m GiB total capacity; \u001b[1;36m22.05\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m49.50\u001b[0m MiB free; \u001b[1;36m22.42\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated \n",
       "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 학습 (10min)\n",
    "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    gradient_accumulation_steps=8,\n",
    "#     per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "#     eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    save_total_limit = 1,\n",
    "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    optim='adafactor',\n",
    "    report_to='none'\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_state()\n",
    "# safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output_1_SFT'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7CFw58BSMVZ",
    "outputId": "b6f8d7ec-ace1-438b-e0f0-d66036e1ff6a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이기 때문에 실제로 불고기를 먹을 수 없습니다. 하지만 일반적으로 불고기용 고기는 건강에 좋은 식품 중 하나입니다. 따라서 건강한 식습관을 유지하는 것이 좋습니다.\\n\\n따라서 어떤 종류의 불고기를 원하시는지 알려주시면 더 정확한 답변을\n",
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 47대 부통령직을 수행했습니다. Canadd of Johnson. There are personal service, but I can assist you would you like to provide more context or\n",
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤이어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다. Communicate of Capability, I am not have service to provide more context or details. Could you please prov\n",
      "######################################################################\n",
      "completion: Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로, 미세먼지 정보를 알 수 없습니다. 하지만 미세먼지 발생 원인은 다양할 수 있습니다. 예를 들어, 호흡기 질환, 심혈관계 질환, 알레르기 질환 등이 있을 수 있습니다. 따라서, 미세먼지 여부를 확인하시려면 해당 장소의 공기청정기 또는 마스크를 착용하시기 바\n"
     ]
    }
   ],
   "source": [
    "## 추론 테스트\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어',\n",
    "               '오늘 미세먼지 어때?']\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print(('#'*70))\n",
    "    print(('completion: %s'%(result[0]['generated_text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uZXNbXG8v4g5"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti9X13R6SMVZ"
   },
   "source": [
    "#### GPT2 모델이 사람의 질문에 대해 **'잘'** 대답하는 모델을 학습했습니다.  \n",
    "#### ``output_1_SFT`` 폴더에 학습된 모델이 저장되어 있습니다.  \n",
    "#### 이제 step2) RM 보상모델을 학습해 볼까요??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V7EindF6SMVZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tOV7g49CSMVZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU4djUZ_7tit"
   },
   "source": [
    "### 2) RM: 좋은 글 채점기 만들기\n",
    "- Collect a human annotated dataset and train a reward model\n",
    "- **배경**\n",
    "    - 기존 AI는 주관적인 글을 채점(점수화) 할 수 없었음\n",
    "    - 사람이 직접 피드백을 줘서 글 채점의 척도로 사용하자\n",
    "    - 매번 사람이 채점할 수 없으니, 사람의 채점을 모방하는 **좋은글 채점 AI모델** 을 만들자\n",
    "\n",
    "    - 채점 AI모델을 만드려면, 사람이 글을 채점한 데이터셋(33,000개)이 필요하다\n",
    "    - 동일 질문에 대해 AI모델이 생성한 여러 글(한 번에 4~6개 세트)을 사람이 직접 ranking을 매긴다.\n",
    "    - 왜?? 사람이 생성한 글에 바로 점수를 매기게 되면 사람마다 기준이 다를 수 있기 때문에 순위로\n",
    "    - **C > B > A**  \n",
    "\n",
    "- **Human labeling 예시**\n",
    "<img src=\"img/2_RM_1.png\" width=\"700\">  \n",
    "\n",
    "\n",
    "- **좋은글 채점 모델 학습(RM, Reward Model)**\n",
    "    - 1등 글은 높은 점수를\n",
    "    - 꼴등 데이터는 낮은 점수를\n",
    "    - 입력: AI가 생성한 글\n",
    "    - 출력: 0~1점  \n",
    "\n",
    "\n",
    "- 보상모델 입출력\n",
    "<img src=\"img/2_RM_2.png\" width=\"700\">\n",
    "\n",
    "- **결과물**\n",
    "    - Before: 좋은 글, 나쁜 글 판단 불가능\n",
    "    - After: 사람이 읽기에 좋은글/나쁜글 판단 모델\n",
    "    \n",
    "    \n",
    "- **전체 구조**\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuXE_BLP8ALx"
   },
   "source": [
    "##### Step 2) Train the reward model\n",
    "-[ref](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/train_reward_model.py)\n",
    "\n",
    "We use rm-static as dataset to train our reward model.\n",
    "It is a dataset of chosen & rejected response of the same prompt.\n",
    "You can download the dataset from huggingface automatically.\n",
    "Use these code to train your reward model.\n",
    "\n",
    "##### Naive reward model training\n",
    "python train_reward_model.py --pretrain <your model path>\n",
    "\n",
    "##### if to use LoRA\n",
    "python train_reward_model.py --pretrain <your model path> --lora_rank 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "KphqHc6lgu38"
   },
   "outputs": [],
   "source": [
    "# ## setup(1min)\n",
    "# # torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"Torch version:{}\".format(torch.__version__))\n",
    "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "# # for ColossalAI\n",
    "# !pip install colossalai==0.2.7\n",
    "\n",
    "# # setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiuVfmvb7tTD",
    "outputId": "fc7095f0-b5dc-4bfe-bccb-0b6a3b24d383"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/library.py:130: UserWarning: Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n",
      "    registered at aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: Meta\n",
      "  previous kernel: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1053\n",
      "       new kernel: registered at /dev/null:241 (Triggered internally at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:150.)\n",
      "  self.m.impl(name, dispatch_key, fn)\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import argparse\n",
    "\n",
    "import loralib as lora\n",
    "import torch\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMRM\n",
    "from chatgpt.models.gpt import GPTRM\n",
    "from chatgpt.models.opt import OPTRM\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNEweAK77tNT",
    "outputId": "ccd6fbec-82d8-4e53-800c-40b2a1cd29f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(output_dir='./output_2_RM', data_path_2_RM='./data_kochatgpt/kochatgpt_2_RM.jsonl', strategy='naive', model='gpt2', pretrain_model='./output_1_SFT', pretrain_tokenizer='skt/kogpt2-base-v2', dataset='Dahoas/rm-static', save_path='rm_ckpt.pth', max_epochs=3, batch_size=4, lora_rank=0, max_len=512, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n",
    "parser.add_argument('--data_path_2_RM', type=str, default='./data_kochatgpt/kochatgpt_2_RM.jsonl', help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain_model', type=str, default=None)\n",
    "parser.add_argument('--pretrain_tokenizer', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
    "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_len', type=int, default=512)  # wygo 추가\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.max_epochs = 3\n",
    "args.pretrain_tokenizer = 'skt/kogpt2-base-v2'  # pretrained 토크나이저 가져오기\n",
    "# args.pretrain_model = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n",
    "args.pretrain_model = './output_1_SFT'  # pretrained 모델 가져오기 (sft 그대로 써야 함)\n",
    "args.verbose = True\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "mtwPLUIq7tIb"
   },
   "outputs": [],
   "source": [
    "# configure strategy\n",
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [],
    "id": "oZ96VsvwnHG9"
   },
   "outputs": [],
   "source": [
    "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
    "from typing import Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "# from ..base import RewardModel\n",
    "from chatgpt.models.base import RewardModel\n",
    "\n",
    "\n",
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT Reward model.\n",
    "    Args:\n",
    "        pretrained (str): Pretrained model name or path.\n",
    "        config (GPT2Config): Model config.\n",
    "        checkpoint (bool): Enable gradient checkpointing.\n",
    "        lora_rank (int): Rank of the low-rank approximation.\n",
    "        lora_train_bias (str): LoRA bias training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        \n",
    "        # model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        # 추가, 230421    \n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "        \n",
    "    # 추가, 230421, config.json을 생성하기 위해 추가\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ee3we4uB8EdK",
    "outputId": "fe1726bf-1a5a-4368-c793-1fee2d9c95bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at ./output_1_SFT were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    # load pretrained gpt2    \n",
    "    if args.model == 'gpt2':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain_tokenizer, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = GPTRM_custom(pretrained=args.pretrain_model, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
    "\n",
    "    elif args.model == 'bloom':\n",
    "        model = BLOOMRM(pretrained=args.pretrain_model, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain_tokenizer)\n",
    "    \n",
    "    elif args.model == 'opt':\n",
    "        model = OPTRM(pretrained=args.pretrain_model, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "    \n",
    "    \n",
    "    # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GD94Zd-b7tDL",
    "outputId": "3a5828e5-3ef5-47df-87ad-2f26d1748663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## data check ##\n",
      "{'prompt': 'Below is an instruction that describes a task.\\n아래는 작업을 설명하는 명령어입니다.\\n\\nWrite a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### Instruction(명령어):\\n번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?\\n\\n### Response(응답):', 'completion_0': 'Allow me to answer your question. I know that you are curious about me.', 'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.', 'completion_2': '라이언에게 말했다.', 'ranking': [2, 1, 0]}\n",
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': 'Below is an instruction that describes a task.\\n아래는 작업을 설명하는 명령어입니다.\\n\\nWrite a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### Instruction(명령어):\\n애플은 리사를 어떻게 처리했어\\n\\n### Response(응답):', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "# make ranking data to chosen, rejetced data\n",
    "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    \n",
    "    # lsw add\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "    list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "    for x,y in zip(list_data_dict, list_prompt):\n",
    "        x.update({'prompt':y})\n",
    "\n",
    "    if args.verbose:\n",
    "        print('## data check ##')\n",
    "        print((list_data_dict[0]))\n",
    "        \n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # data 1) 0 VS 1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    # data 2) 0 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # data 1) 1 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWzG_w3d7s9h",
    "outputId": "05ce4b31-5f97-4cd1-a0d1-8ae1b7e4f326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Below is an instruction that describes a task.\\n아래는 작업을 설명하는 명령어입니다.\\n\\nWrite a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### Instruction(명령어):\\n유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?\\n\\n### Response(응답):', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1229.50it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 1372.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "체크인 되나요?\n",
      "\n",
      "### Response(응답):\n",
      "######################################################################\n",
      "## chosen ##\n",
      "제가 AI 챗봇이기 때문에 호텔이나 항공편 등으로 어떤 체크인을 말씀하시는 것인지 구체적으로 설명해주시면 답변을 드리겠습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "다시 한번 가지게임이지 않아 가지게임이지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare for data and dataset\n",
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 29000 학습\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 1000개만 평가\n",
    "\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "OQI5a1hm7s3L"
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "if args.strategy.startswith('colossalai'):\n",
    "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
    "else:\n",
    "    optim = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "97mbryrTgXJ0"
   },
   "outputs": [],
   "source": [
    "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
    "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=strategy,\n",
    "                             optim=optim,\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             max_epochs=args.max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE6HOKdmgY6X"
   },
   "source": [
    "###### 참고\n",
    "- [train reward model](https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/trainer/rm.py#L68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpOYBhbH8HLh",
    "outputId": "32806121-5413-43fc-ea30-15afc3a940c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 1/25 [00:00<00:23,  1.04it/s]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 1/25 [00:00<00:23,  1.04it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 2/25 [00:01<00:13,  1.69it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 2/25 [00:01<00:13,  1.69it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 3/25 [00:01<00:09,  2.21it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 3/25 [00:01<00:09,  2.21it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 4/25 [00:01<00:08,  2.59it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 4/25 [00:01<00:08,  2.59it/s, loss=0.47] \u001b[A\n",
      "Train step of epoch 0:  20%|██        | 5/25 [00:02<00:07,  2.85it/s, loss=0.47]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 5/25 [00:02<00:07,  2.85it/s, loss=0.266]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 6/25 [00:02<00:06,  3.04it/s, loss=0.266]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 6/25 [00:02<00:06,  3.04it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 7/25 [00:02<00:05,  3.17it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 7/25 [00:02<00:05,  3.17it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 8/25 [00:03<00:05,  3.27it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 8/25 [00:03<00:05,  3.27it/s, loss=2.69] \u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 9/25 [00:03<00:04,  3.34it/s, loss=2.69]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 9/25 [00:03<00:04,  3.34it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 10/25 [00:03<00:04,  3.38it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 10/25 [00:03<00:04,  3.38it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 11/25 [00:03<00:04,  3.41it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 11/25 [00:03<00:04,  3.41it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 12/25 [00:04<00:03,  3.44it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 12/25 [00:04<00:03,  3.44it/s, loss=0.54] \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 13/25 [00:04<00:03,  3.45it/s, loss=0.54]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 13/25 [00:04<00:03,  3.45it/s, loss=0.351]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 14/25 [00:04<00:03,  3.46it/s, loss=0.351]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 14/25 [00:04<00:03,  3.46it/s, loss=0.831]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 15/25 [00:05<00:02,  3.47it/s, loss=0.831]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 15/25 [00:05<00:02,  3.47it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 16/25 [00:05<00:02,  3.48it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 16/25 [00:05<00:02,  3.48it/s, loss=1.99] \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 17/25 [00:05<00:02,  3.48it/s, loss=1.99]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 17/25 [00:05<00:02,  3.48it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 18/25 [00:05<00:02,  3.48it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 18/25 [00:05<00:02,  3.48it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 19/25 [00:06<00:01,  3.49it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 19/25 [00:06<00:01,  3.49it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 20/25 [00:06<00:01,  3.49it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 20/25 [00:06<00:01,  3.49it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 21/25 [00:06<00:01,  3.49it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 21/25 [00:06<00:01,  3.49it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 22/25 [00:07<00:00,  3.49it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 22/25 [00:07<00:00,  3.49it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 23/25 [00:07<00:00,  3.49it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 23/25 [00:07<00:00,  3.49it/s, loss=1.2]  \u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 24/25 [00:07<00:00,  3.49it/s, loss=1.2]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 24/25 [00:07<00:00,  3.49it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 25/25 [00:07<00:00,  3.49it/s, loss=0.767]\u001b[A\n",
      "Train epoch:  33%|███▎      | 1/3 [00:08<00:17,  8.57s/it],  3.49it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 25/25 [00:08<00:00,  2.92it/s, loss=0.596, dist_mean=0.282]\u001b[A\n",
      "\n",
      "Train step of epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:06,  3.62it/s]\u001b[A\n",
      "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:06,  3.62it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 2/25 [00:00<00:06,  3.54it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 2/25 [00:00<00:06,  3.54it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 3/25 [00:00<00:06,  3.51it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 3/25 [00:00<00:06,  3.51it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 1:  16%|█▌        | 4/25 [00:01<00:05,  3.50it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 1:  16%|█▌        | 4/25 [00:01<00:05,  3.50it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 1:  20%|██        | 5/25 [00:01<00:05,  3.50it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 1:  20%|██        | 5/25 [00:01<00:05,  3.50it/s, loss=0.46] \u001b[A\n",
      "Train step of epoch 1:  24%|██▍       | 6/25 [00:01<00:05,  3.50it/s, loss=0.46]\u001b[A\n",
      "Train step of epoch 1:  24%|██▍       | 6/25 [00:01<00:05,  3.50it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 7/25 [00:01<00:05,  3.49it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 7/25 [00:02<00:05,  3.49it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 8/25 [00:02<00:04,  3.49it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 8/25 [00:02<00:04,  3.49it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 1:  36%|███▌      | 9/25 [00:02<00:04,  3.49it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 1:  36%|███▌      | 9/25 [00:02<00:04,  3.49it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 1:  40%|████      | 10/25 [00:02<00:04,  3.48it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 1:  40%|████      | 10/25 [00:02<00:04,  3.48it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 1:  44%|████▍     | 11/25 [00:03<00:04,  3.47it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 1:  44%|████▍     | 11/25 [00:03<00:04,  3.47it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 12/25 [00:03<00:03,  3.47it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 12/25 [00:03<00:03,  3.47it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 13/25 [00:03<00:03,  3.47it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 13/25 [00:03<00:03,  3.47it/s, loss=0.121]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▌    | 14/25 [00:04<00:03,  3.47it/s, loss=0.121]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▌    | 14/25 [00:04<00:03,  3.47it/s, loss=0.984]\u001b[A\n",
      "Train step of epoch 1:  60%|██████    | 15/25 [00:04<00:02,  3.48it/s, loss=0.984]\u001b[A\n",
      "Train step of epoch 1:  60%|██████    | 15/25 [00:04<00:02,  3.48it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▍   | 16/25 [00:04<00:02,  3.47it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▍   | 16/25 [00:04<00:02,  3.47it/s, loss=1.81] \u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 17/25 [00:04<00:02,  3.47it/s, loss=1.81]\u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 17/25 [00:04<00:02,  3.47it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 18/25 [00:05<00:02,  3.48it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 18/25 [00:05<00:02,  3.48it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▌  | 19/25 [00:05<00:01,  3.48it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▌  | 19/25 [00:05<00:01,  3.48it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 1:  80%|████████  | 20/25 [00:05<00:01,  3.48it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 1:  80%|████████  | 20/25 [00:05<00:01,  3.48it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▍ | 21/25 [00:06<00:01,  3.48it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▍ | 21/25 [00:06<00:01,  3.48it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 22/25 [00:06<00:00,  3.48it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 22/25 [00:06<00:00,  3.48it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 23/25 [00:06<00:00,  3.48it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 23/25 [00:06<00:00,  3.48it/s, loss=0.961]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▌| 24/25 [00:06<00:00,  3.48it/s, loss=0.961]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▌| 24/25 [00:06<00:00,  3.48it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 1: 100%|██████████| 25/25 [00:07<00:00,  3.48it/s, loss=0.383]\u001b[A\n",
      "Train epoch:  67%|██████▋   | 2/3 [00:16<00:08,  8.16s/it],  3.48it/s, loss=0.297]\u001b[A\n",
      "Train step of epoch 1: 100%|██████████| 25/25 [00:07<00:00,  3.18it/s, loss=0.508, dist_mean=0.715]\u001b[A\n",
      "\n",
      "Train step of epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:06,  3.60it/s]\u001b[A\n",
      "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:06,  3.60it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 2/25 [00:00<00:06,  3.53it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 2/25 [00:00<00:06,  3.53it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 3/25 [00:00<00:06,  3.51it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 3/25 [00:00<00:06,  3.51it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 2:  16%|█▌        | 4/25 [00:01<00:06,  3.49it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 2:  16%|█▌        | 4/25 [00:01<00:06,  3.49it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 2:  20%|██        | 5/25 [00:01<00:05,  3.49it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 2:  20%|██        | 5/25 [00:01<00:05,  3.49it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 2:  24%|██▍       | 6/25 [00:01<00:05,  3.48it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 2:  24%|██▍       | 6/25 [00:01<00:05,  3.48it/s, loss=0.923]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 7/25 [00:02<00:05,  3.48it/s, loss=0.923]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 7/25 [00:02<00:05,  3.48it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 8/25 [00:02<00:04,  3.48it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 8/25 [00:02<00:04,  3.48it/s, loss=0.317]\u001b[A\n",
      "Train step of epoch 2:  36%|███▌      | 9/25 [00:02<00:04,  3.47it/s, loss=0.317]\u001b[A\n",
      "Train step of epoch 2:  36%|███▌      | 9/25 [00:02<00:04,  3.47it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  40%|████      | 10/25 [00:02<00:04,  3.47it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  40%|████      | 10/25 [00:02<00:04,  3.47it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 2:  44%|████▍     | 11/25 [00:03<00:04,  3.47it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 2:  44%|████▍     | 11/25 [00:03<00:04,  3.47it/s, loss=0.223]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 12/25 [00:03<00:03,  3.46it/s, loss=0.223]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 12/25 [00:03<00:03,  3.46it/s, loss=0.403]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 13/25 [00:03<00:03,  3.46it/s, loss=0.403]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 13/25 [00:03<00:03,  3.46it/s, loss=0.0369]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▌    | 14/25 [00:04<00:03,  3.47it/s, loss=0.0369]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▌    | 14/25 [00:04<00:03,  3.47it/s, loss=0.484] \u001b[A\n",
      "Train step of epoch 2:  60%|██████    | 15/25 [00:04<00:02,  3.47it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 2:  60%|██████    | 15/25 [00:04<00:02,  3.47it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 2:  64%|██████▍   | 16/25 [00:04<00:02,  3.47it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 2:  64%|██████▍   | 16/25 [00:04<00:02,  3.47it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 17/25 [00:04<00:02,  3.47it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 17/25 [00:04<00:02,  3.47it/s, loss=0.0221]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 18/25 [00:05<00:02,  3.47it/s, loss=0.0221]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 18/25 [00:05<00:02,  3.47it/s, loss=0.158] \u001b[A\n",
      "Train step of epoch 2:  76%|███████▌  | 19/25 [00:05<00:01,  3.47it/s, loss=0.158]\u001b[A\n",
      "Train step of epoch 2:  76%|███████▌  | 19/25 [00:05<00:01,  3.47it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 2:  80%|████████  | 20/25 [00:05<00:01,  3.47it/s, loss=0.565]\u001b[A\n",
      "Train step of epoch 2:  80%|████████  | 20/25 [00:05<00:01,  3.47it/s, loss=0.213]\u001b[A\n",
      "Train step of epoch 2:  84%|████████▍ | 21/25 [00:06<00:01,  3.47it/s, loss=0.213]\u001b[A\n",
      "Train step of epoch 2:  84%|████████▍ | 21/25 [00:06<00:01,  3.47it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 22/25 [00:06<00:00,  3.47it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 22/25 [00:06<00:00,  3.47it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 23/25 [00:06<00:00,  3.47it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 23/25 [00:06<00:00,  3.47it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▌| 24/25 [00:06<00:00,  3.47it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▌| 24/25 [00:06<00:00,  3.47it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 2: 100%|██████████| 25/25 [00:07<00:00,  3.47it/s, loss=0.143]\u001b[A\n",
      "Train epoch: 100%|██████████| 3/3 [00:24<00:00,  8.03s/it],  3.47it/s, loss=0.137]\u001b[A\n",
      "Train step of epoch 2: 100%|██████████| 25/25 [00:07<00:00,  3.17it/s, loss=0.751, dist_mean=0.689]\u001b[A\n",
      "Train epoch: 100%|██████████| 3/3 [00:24<00:00,  8.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# train!!\n",
    "trainer.fit(use_lora=args.lora_rank)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(optim,\n",
    "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)\n",
    "\n",
    "model.save_pretrained(args.output_dir)  # config.json 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaGv2_qUsRrb",
    "outputId": "721873d7-4ba5-49e3-b83b-fef5bd31e2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 한국은 대한민국 입니다\n",
      "reward score: -8.1\n"
     ]
    }
   ],
   "source": [
    "# 보상모델 체크\n",
    "def inference_RM(input_text='인공지능은 인공지능 입니다'):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "    \n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "    \n",
    "    return output_reward\n",
    "\n",
    "\n",
    "input_text = '한국은 대한민국 입니다'\n",
    "# input_text = '인공지능은 인공지능 입니다'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Qt21yMWCv1S0"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-sht1EoAt1S"
   },
   "source": [
    "#### 사람의 선호도를 모방한 좋은 글 채점기 모델을 학습했습니다.  \n",
    "#### ``output_2_RM`` 폴더에 학습된 모델이 저장되어 있습니다.  \n",
    "#### 이제 step3) PPO 모델을 학습해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G_nNydSKAtOu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4IFUPxId7sa3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqkr7S_-pO9m"
   },
   "source": [
    "### Step 3) 사람의 피드백을 반영하여 학습\n",
    "\n",
    "- Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (e.g. PPO)\n",
    "- 배경\n",
    "    - **사람의 순위를 모사한 보상모델(RM)** 의 점수가 높아지도록 학습 (31,000개)\n",
    "    - 초기 모델에 비해 너무 많이 바뀌지 않도록  \n",
    "    \n",
    "    \n",
    "    \n",
    "<img src=\"./img/3_PPO_1.png\" width=\"650\">\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "EQd6DxzgpOdz",
    "outputId": "c50152fd-5fb1-4ca8-c224-3b38909fa235"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/oC7Cw3fu3gU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fb40f1ace50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human Feedback RL\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('oC7Cw3fu3gU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "m2ZFYClBpTol"
   },
   "source": [
    "##### RLHF\n",
    "- [train_dummy](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/train_dummy.py)\n",
    "- [train_prompts](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/train_prompts.py)\n",
    "- [좋은설명](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093)\n",
    "\n",
    "- [dataset1](https://huggingface.co/datasets/Dahoas/rm-static)\n",
    "- [dataset2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/chatgpt.png\" width=\"500\">\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- Fine-tuning 태스크를 강화학습 문제로 다음과 같이 정형화\n",
    "    - Policy : 언어모델 - 프롬프트를 입력으로 받아 텍스트의 시퀀스(혹은 그 확률)를 리턴\n",
    "    - Action space : 언어모델의 모든 단어 (일반적으로 5만개 분량)\n",
    "    - Observation space : 가능한 인풋 토큰 시퀀스 (단어개수^시퀀스길이 이므로 엄청 큼!)\n",
    "    - Reward function : 보상모델과 policy shift에 대한 제약조건의 조합으로 정의됨\n",
    "\n",
    "<img src=\"img/3_PPO_2.png\" width=\"500\">\n",
    "\n",
    "- Frozen Model과 Non-frozen(trainable) Model의 텍스트 출력 확률간 KL divergence를 계산\n",
    "- trainable Model의 weight가 완전히 바뀌는 것을 방지하고 Reward Model에 말도 되지 않는 텍스트로 출력을 시작하는 것을 방지\n",
    "\n",
    "\n",
    "<img src=\"img/3_PPO_3.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- PPO process\n",
    "[1] 초기화를 위해 intial probs(initial output text probabilities)를 new probs(new output text probabilities)와 동일하게 만듬\n",
    "\n",
    "- while:\n",
    "    - [2] New probs와 initial probs간 ratio을 계산함\n",
    "    - [3] 아래 공식에 따라 loss를 계산함.\n",
    "        - loss = -min(ratio * R, clip(ratio, 0.8, 1.2) * R)\n",
    "            - R = reward + KL (or 0.8*reward + 0.2*KL와 같은 weighted average)\n",
    "            - clip(ratio, 0.8, 1.2) → 0.8 ≤ ratio ≤ 1.2\n",
    "    - [4] Loss를 backpropagating하여 SFT Model의 weight를 업데이트함\n",
    "\n",
    "    - [5] 새롭게 업데이트된 SFT 모델로 new probs를 계산함\n",
    "\n",
    "    - [6] 2번부터 6번을 N 번 반복함\n",
    "\n",
    "\n",
    "\n",
    "- [loss1](https://github.com/hpcaitech/ColossalAI/blob/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/experience_maker/naive.py#L7)\n",
    "- [loss2](https://github.com/hpcaitech/ColossalAI/blob/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/models/utils.py#L31)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "-fo6KWWcpdxh"
   },
   "outputs": [],
   "source": [
    "# ## setup(1min)\n",
    "# # torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(\"Torch version:{}\".format(torch.__version__))\n",
    "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "# # for ColossalAI\n",
    "# !pip install colossalai==0.2.7\n",
    "\n",
    "# # setup data\n",
    "# !git clone https://github.com/airobotlab/KoChatGPT\n",
    "# !mv KoChatGPT/data_kochatgpt .\n",
    "# !mv KoChatGPT/img .\n",
    "\n",
    "# %cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "# !pip install .\n",
    "# %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "E3Ytmwj-diQ6"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.models.opt import OPTActor, OPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "\n",
    "## wy 추가\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## clossalAI error 해결\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '2'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '42043'\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\"\n",
    "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Ctd5oDVCdiJJ",
    "outputId": "f420a335-886d-4eb3-f549-2adddd4913ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path_3_PPO='./data_kochatgpt/kochatgpt_3_PPO.jsonl', output_dir='./output_3_PPO', strategy='naive', model='gpt2', pretrain='skt/kogpt2-base-v2', num_episodes=1, max_timesteps=3, update_timesteps=3, max_epochs=5, train_batch_size=8, lora_rank=0, max_length=250, pretrain_actor='./output_1_SFT', pretrain_critic='./output_2_RM')\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_3_PPO', type=str, default='./data_kochatgpt/kochatgpt_3_PPO.jsonl')\n",
    "parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--num_episodes', type=int, default=10)\n",
    "parser.add_argument('--max_timesteps', type=int, default=3)\n",
    "parser.add_argument('--update_timesteps', type=int, default=3)\n",
    "parser.add_argument('--max_epochs', type=int, default=5)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.output_dir = './output_3_PPO'\n",
    "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n",
    "\n",
    "\n",
    "## 이곳 수정!!\n",
    "args.pretrain_actor = './output_1_SFT'  # SFT 모델 가져오기\n",
    "args.pretrain_critic = './output_2_RM'  # RM 모델 가져오기\n",
    "# args.pretrain_actor = args.pretrain\n",
    "# args.pretrain_critic = args.pretrain\n",
    "\n",
    "args.num_episodes = 1\n",
    "args.max_epochs   = 5\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "E5YEhGiIdiBQ"
   },
   "outputs": [],
   "source": [
    "# configure strategy\n",
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "2ZJ4oN9BdnL6",
    "outputId": "2d0ed508-fa6b-471c-c329-a586ddf1621f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    if args.model == 'gpt2':\n",
    "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )    \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "#     elif args.model == 'bloom':\n",
    "#         actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
    "#         tokenizer.pad_token = tokenizer.eos_token            \n",
    "#     elif args.model == 'opt':\n",
    "#         actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "3OnJ76tgdnAr"
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "if args.strategy.startswith('colossalai'):\n",
    "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
    "else:\n",
    "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "3CPTGw5TeW9p"
   },
   "outputs": [],
   "source": [
    "# setting the models\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "jM3Tm4Lhdh5U",
    "outputId": "131194ca-a868-41c2-9439-39838990d22f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "# lsw add\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(list_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(tokenize_fn('I want you to act as a linux terminal.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(51200, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTCritic(\n",
       "  (model): GPT2Model(\n",
       "    (wte): Embedding(51201, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (model): GPT2Model(\n",
       "    (wte): Embedding(51201, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model.training, actor.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140217472857472, 140217473349376, 140217472857856, 140217483856960)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(initial_model), id(reward_model), id(actor), id(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "DAS01J_iedt5",
    "outputId": "395b8d38-0950-4ca0-c6be-bd46747cd2b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/1]:  67%|██████▋   | 2/3 [01:01<00:30, 30.12s/it]\n",
      "Train epoch [1/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0323]\u001b[A\n",
      "Train epoch [1/5]:  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s, actor_loss=0, critic_loss=0.0323]\u001b[A\n",
      "Train epoch [1/5]:  33%|███▎      | 1/3 [00:01<00:01,  1.60it/s, actor_loss=0, critic_loss=1.39]  \u001b[A\n",
      "Train epoch [1/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s, actor_loss=0, critic_loss=1.39]\u001b[A\n",
      "Train epoch [1/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.60it/s, actor_loss=0, critic_loss=0.0949]\u001b[A\n",
      "Train epoch [1/5]: 100%|██████████| 3/3 [00:01<00:00,  1.60it/s, actor_loss=0, critic_loss=0.0949]\u001b[A\n",
      "\n",
      "Train epoch [2/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.223]\u001b[A\n",
      "Train epoch [2/5]:  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s, actor_loss=0, critic_loss=0.223]\u001b[A\n",
      "Train epoch [2/5]:  33%|███▎      | 1/3 [00:01<00:01,  1.62it/s, actor_loss=0, critic_loss=0.646]\u001b[A\n",
      "Train epoch [2/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.646]\u001b[A\n",
      "Train epoch [2/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.679]\u001b[A\n",
      "Train epoch [2/5]: 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.679]\u001b[A\n",
      "\n",
      "Train epoch [3/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0821]\u001b[A\n",
      "Train epoch [3/5]:  33%|███▎      | 1/3 [00:00<00:01,  1.62it/s, actor_loss=0, critic_loss=0.0821]\u001b[A\n",
      "Train epoch [3/5]:  33%|███▎      | 1/3 [00:01<00:01,  1.62it/s, actor_loss=0, critic_loss=0.0293]\u001b[A\n",
      "Train epoch [3/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.62it/s, actor_loss=0, critic_loss=0.0293]\u001b[A\n",
      "Train epoch [3/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.62it/s, actor_loss=0, critic_loss=0.0679]\u001b[A\n",
      "Train epoch [3/5]: 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.0679]\u001b[A\n",
      "\n",
      "Train epoch [4/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [4/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.299]\u001b[A\n",
      "Train epoch [4/5]:  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s, actor_loss=0, critic_loss=0.299]\u001b[A\n",
      "Train epoch [4/5]:  33%|███▎      | 1/3 [00:01<00:01,  1.61it/s, actor_loss=0, critic_loss=0.342]\u001b[A\n",
      "Train epoch [4/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.342]\u001b[A\n",
      "Train epoch [4/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.238]\u001b[A\n",
      "Train epoch [4/5]: 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.238]\u001b[A\n",
      "\n",
      "Train epoch [5/5]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [5/5]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0715]\u001b[A\n",
      "Train epoch [5/5]:  33%|███▎      | 1/3 [00:00<00:01,  1.61it/s, actor_loss=0, critic_loss=0.0715]\u001b[A\n",
      "Train epoch [5/5]:  33%|███▎      | 1/3 [00:01<00:01,  1.61it/s, actor_loss=0, critic_loss=0.0408]\u001b[A\n",
      "Train epoch [5/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.0408]\u001b[A\n",
      "Train epoch [5/5]:  67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.107] \u001b[A\n",
      "Train epoch [5/5]: 100%|██████████| 3/3 [00:01<00:00,  1.61it/s, actor_loss=0, critic_loss=0.107]\u001b[A\n",
      "Episode [1/1]: 100%|██████████| 3/3 [01:38<00:00, 32.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# configure trainer\n",
    "trainer = PPOTrainer(strategy,\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=args.max_epochs,\n",
    "                     train_batch_size=args.train_batch_size,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=512,\n",
    "#                      max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "## train!\n",
    "trainer.fit(list_prompt,  # 입력 prompt\n",
    "            num_episodes=args.num_episodes,\n",
    "            max_timesteps=args.max_timesteps,\n",
    "            update_timesteps=args.update_timesteps)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(actor_optim,\n",
    "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '</s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '</s>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "mjmSYfy2xBi5",
    "outputId": "7d267181-a921-4bdd-d2d7-7cdfbda4767a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능이기 때문에, 버리는 고기용 육수에 대한 정보는 가지고 있지 않습니다. 하지만, 일반적으로 불고기용 육소는 매운맛을 좋아하는 사람들이 좋아하는 음식거리 중 하나이며, 보통 부드러운 소스에 많이 사용됩니다. 하지만 맛과 향으로 인한 불레기 문제로 불고기가 질척하는 상황이 발생할수도 있습니다. 맛과 건강 상태에 대한 지속적인 관리와 함께 상태에 대해 주의가 필요합니다.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 작성해줘.\n",
      "\n",
      "### Response(응답):'제가는 인공지능 언어모델로써 인간의 자격이나 지위만을 가지고 ISA가입자를 직접 행위하지 않습니다. 이는 잘못된 질문인 것으로 간주됩니다. 이러한 문제는 모든 관계에서 잘 알고 있으며, 가능한 많은 도움이 필요합니다. ISA가입 관련하여 여러 가지 물건에 대한 정보가 필요하다면, ISA가입 캠페인을 통해 가입자 신원이나 환경을 평가하고, 가입자 자격을 증명할 수 있도록 돕는 것이 중요합니다. 잘못된 정보는 적극적으로 삭제하시고, 건강한 가입을 위해 충분한 정보나 활동 정보가 필요합니다.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "한국에서 제일 유명한 가수와 그 대표곡을 알려줘.\n",
      "\n",
      "### Response(응답):'CDM (Chemistry Delta)\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "농심에서 출시한 라면 5가지와 그 특징을 알려줘\n",
      "\n",
      "### Response(응답):'농심에서 출시한 라면은 여러가지 요소, 형태, 제조 방법 등이 있습니다. 각 종류는 밀가루, 소스, 소스, 소스, 국물 등이 특징입니다. 그 외에도 면을 뿌려 만든 맛있는 라면, 육전 반죽을 사용한 안도라 고구마 등 다양한 종류가 있습니다. 또한 여러 가지 사이드 메뉴도 있습니다.  \\n \\n- 적용된 치킨 종류에 따라 돼지고기 등심이 참깨,토바이월드 등 다양한 종류의 라면을 제공합니다. \\n- 조리방법>\\n- 육전에서 최초로 고안한 평어로 만들어진, 고기나 양파, 썬귤 등이 대표적입니다. \\n- 제품 이름으로는 돼지 목살라,개와 계약취 이렇게 여러가지 종류가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# lsw added\n",
    "# eos token 이후 문장 생성 방지용 \n",
    "actor.pad_token_id = tokenizer.pad_token_id\n",
    "actor.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args.max_length,\n",
    "                             do_sample=True,\n",
    "#                              top_k=50,\n",
    "#                              top_p=0.95,\n",
    "                             temprature = 0.8,\n",
    "                             no_repeat_ngram_size=6,\n",
    "                             pad_token_id = tokenizer.pad_token_id,\n",
    "                             eos_token_id = tokenizer.eos_token_id,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "#     '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "#     '시카고 오헤어 국제공항은 어디에 있어',\n",
    "#     '오늘 미세먼지 어때?'\n",
    "    '직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 작성해줘.',\n",
    "    '한국에서 제일 유명한 가수와 그 대표곡을 알려줘.',\n",
    "    '농심에서 출시한 라면 5가지와 그 특징을 알려줘'\n",
    "]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTPxdCO0A6yt"
   },
   "source": [
    "#### 좋은글 생성기와 좋은글 채점기로 강화학습을 하여 ChatGPT-replica를 학습했습니다.  \n",
    "#### ``output_3_PPO`` 폴더에 학습된 모델이 저장되어 있습니다.  \n",
    "#### 끝~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NY84CFCvA6N-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hvp1QP0VA582"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "ihMgjx60hCHM"
   },
   "source": [
    "### inference PPO actor\n",
    "- [ref](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ChatGPT/examples/inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "id": "PcRr_cZThBqa"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from chatgpt.models.bloom import BLOOMActor\n",
    "from chatgpt.models.gpt import GPTActor\n",
    "from chatgpt.models.opt import OPTActor\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\":\n",
    "    (\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "     \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "     \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "     ),\n",
    "    \"prompt_no_input\":\n",
    "    (\"Below is an instruction that describes a task.\\n\"\n",
    "     \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
    "     \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
    "     \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "o_hA3_OChEwB",
    "outputId": "380df41b-915b-435b-e073-f438c167e8d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model',\n",
    "                    default='gpt2',\n",
    "                    choices=['gpt2', 'bloom', 'opt'])\n",
    "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--model_path', type=str, default=None)\n",
    "parser.add_argument('--input',\n",
    "                    type=str,\n",
    "                    default='Question: How are you ? Answer:')\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args_inference = parser.parse_args([])\n",
    "\n",
    "args_inference.model = 'gpt2'\n",
    "args_inference.pretrain = 'skt/kogpt2-base-v2'\n",
    "args_inference.model_directory = './output_3_PPO'\n",
    "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
    "\n",
    "# configure model, tokenizer\n",
    "if args_inference.model == 'gpt2':\n",
    "    actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
    "                                              padding_side=\"right\",\n",
    "                                              model_max_length=512)\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    })\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "elif args_inference.model == 'bloom':\n",
    "    actor = BLOOMActor(pretrained=args_inference.pretrain).to(\n",
    "        torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "elif args_inference.model == 'opt':\n",
    "    actor = OPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported model \"{args_inference.model}\"')\n",
    "\n",
    "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
    "actor.model.load_state_dict(state_dict);\n",
    "\n",
    "actor.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'음식물 주문 시 해당 음식점의 메뉴와 사이즈를 알려주시면 답변드릴 수 없습니다. 주문하시는 식당에 문의하시거나, 온라인 마켓플레이스에서 직접 주문하시는 것이 좋습니다.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 작성해줘.\n",
      "\n",
      "### Response(응답):'ISA계좌 판매를 촉진하기 위해서는 해당 조직의 직원이나 기자의 면밀한 검토 등을 통한 정보 수집이 필요합니다. 따라서 해당 회사의 직원이 실제로 ISA계좌 판매를 맡고 있다면, 그 이후에 가입하는 것이 가장 좋습니다. 기숙사나 학교 등에서는 해당 정보를 활용한 안내를 제공하거나, 직원 통관리, 계약 종료 등 후속 조치를 시행합니다. 또한 해당 기관의 홈페이지를 통해 ISA 계좌 판매를 직접 확인할 수 있습니다.\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "한국에서 제일 유명한 가수와 그 대표곡을 알려줘.\n",
      "\n",
      "### Response(응답):'좋습니다. 한국 가수가 몇 개 수록되어 있는지에 따라 다릅니다. 명곡은 다음과 같습니다.\\n\\n1. 팝뮤지션 Lot I Would\\n1. 윤색가수 lot sugar\\n2. 코리안컬렉션 Korean Infoyd\\n3. RMPSK물결 U\\n4. MC 몽키 Fare\\n5. DVD 걸작 시리즈s of chambine\\n6. 브루마쉬kissang Branch\\n7. 킹 오브 바빌라\\n9. 산이여周IIT Wedshow\\n10. Wind MON 9. CHP White Luk\n",
      "######################################################################\n",
      "Below is an instruction that describes a task.\n",
      "아래는 작업을 설명하는 명령어입니다.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Instruction(명령어):\n",
      "농심에서 출시한 라면 5가지와 그 특징을 알려줘\n",
      "\n",
      "### Response(응답):'농심의 라면 5대칙(1) 원활한 유통: 원활한 식습관, 건강한 식습관, 소통관리 잘하고 충분한 수면 유지, 건강한 식습관 등)\\n\\n2) 청결한 식재료를 사용하여 신선한 라면을 만들어 사용\\n\\n3) 지속적인 건강 유지: 규칙적인 생활습관을 유지하고, 수분 공급을 잘 지키며, 영양 균형 있는 식습관을 유지한다.\\n\\n5) 맛있게 먹는 것이 중요하며 건강한 식습관을 유지하며, 건강한 식습관을 유지하는 것도 중요하다.\\n\\n위와 같이 라면을 먹을 때는 식습관을 유지하는 것이 매우 중요한하며, 식품 자체가 건강에 좋지 않은 것은 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# lsw added\n",
    "# eos token 이후 문장 생성 방지용 \n",
    "actor.pad_token_id = tokenizer.pad_token_id\n",
    "actor.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args_inference.max_length,\n",
    "                             do_sample=True,\n",
    "#                              top_k=50,\n",
    "#                              top_p=0.95,\n",
    "                             temprature = 0.8,\n",
    "                             no_repeat_ngram_size=6,\n",
    "                             pad_token_id = tokenizer.pad_token_id,\n",
    "                             eos_token_id = tokenizer.eos_token_id,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print('#' * 70)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "#     '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "#     '시카고 오헤어 국제공항은 어디에 있어',\n",
    "#     '오늘 미세먼지 어때?'\n",
    "    '직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 작성해줘.',\n",
    "    '한국에서 제일 유명한 가수와 그 대표곡을 알려줘.',\n",
    "    '농심에서 출시한 라면 5가지와 그 특징을 알려줘'\n",
    "]\n",
    "\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "G205eduIAyTw"
   },
   "source": [
    "#### 좋은글 생성기와 좋은글 채점기로 강화학습을 하여 ChatGPT-replica를 학습했습니다.  \n",
    "#### ``output_3_PPO`` 폴더에 학습된 모델이 저장되어 있습니다.  \n",
    "#### 끝~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true,
    "id": "Cx52ECfOPalV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_YCMEVRj3rp"
   },
   "source": [
    "# END!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "vHgdUwoJPalV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Wp82sZANSMVa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ecd2ec82a804860a78885c97fc250b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13035ae8dc2741a8bd0a96c234300af2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c2de103392e48ba954c8df37612b395": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_975c6155ea2c476684ea2b6df5483e1b",
      "max": 2825034,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c20354dbe20844c28fe1455c765db2f6",
      "value": 2825034
     }
    },
    "1d0e339ffba14e8d91368770385f1db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9578bba68cfa4c03a21d6251c20053d5",
       "IPY_MODEL_36d0515d2c2847868b3d9dd2ea05f25a",
       "IPY_MODEL_bbd8f769a05048bca99d1075bfe47b6e"
      ],
      "layout": "IPY_MODEL_fa47d352268a4be388b820f9a4a6c5a5"
     }
    },
    "2e3e77f407bc48279570721105a621ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3532edab6de845b589a83f395917578f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e1646bc324c40428c9fe9b7ab8a6921",
       "IPY_MODEL_1c2de103392e48ba954c8df37612b395",
       "IPY_MODEL_56fca42756f94eabaf830bc86d18c4a7"
      ],
      "layout": "IPY_MODEL_4cb558fcf41c4406b15c98af203bf272"
     }
    },
    "36d0515d2c2847868b3d9dd2ea05f25a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f541cd5c2c704129855a39d1c81a48b3",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7b37f9f9d764172828cb005d2b9f633",
      "value": 1000
     }
    },
    "3c309d97eb5d43c798d6db8f49a8e434": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cb558fcf41c4406b15c98af203bf272": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56fca42756f94eabaf830bc86d18c4a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13035ae8dc2741a8bd0a96c234300af2",
      "placeholder": "​",
      "style": "IPY_MODEL_3c309d97eb5d43c798d6db8f49a8e434",
      "value": " 2.83M/2.83M [00:00&lt;00:00, 13.4MB/s]"
     }
    },
    "59d060b9aa65450c8828de299b2b3498": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_674175399071436b8da7cd07ebaa29c8",
      "max": 513302779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c03b82f323e45dbaab32148f00a6938",
      "value": 513302779
     }
    },
    "674175399071436b8da7cd07ebaa29c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7180bc85cb0b4c20b65365c5e84767d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78ff1615fa4b4d8ab51c4683a43f4182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a12232f55c5447fb0284ca001cc0023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "930bd459e2794a32ba4c303b31aaf159": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7180bc85cb0b4c20b65365c5e84767d4",
      "placeholder": "​",
      "style": "IPY_MODEL_93479918dcbd40df9a733045e116a90b",
      "value": " 513M/513M [00:05&lt;00:00, 86.0MB/s]"
     }
    },
    "93479918dcbd40df9a733045e116a90b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9578bba68cfa4c03a21d6251c20053d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ecd2ec82a804860a78885c97fc250b2",
      "placeholder": "​",
      "style": "IPY_MODEL_c677b05ae99841c6879009aa20f2e5f2",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "95adfba856d54c50a4e1d71b8b4a3308": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "975c6155ea2c476684ea2b6df5483e1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c03b82f323e45dbaab32148f00a6938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e1646bc324c40428c9fe9b7ab8a6921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95adfba856d54c50a4e1d71b8b4a3308",
      "placeholder": "​",
      "style": "IPY_MODEL_2e3e77f407bc48279570721105a621ed",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "a6fc9ca6dd874b3b80fe9aa732ec5d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cab874fdbbc0434f876661f578f3f855",
       "IPY_MODEL_59d060b9aa65450c8828de299b2b3498",
       "IPY_MODEL_930bd459e2794a32ba4c303b31aaf159"
      ],
      "layout": "IPY_MODEL_b887240c83db4b77b8bb8391ba0364b4"
     }
    },
    "b7b37f9f9d764172828cb005d2b9f633": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b887240c83db4b77b8bb8391ba0364b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbd8f769a05048bca99d1075bfe47b6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d50332475bbe429f9ef61ba26609cdc6",
      "placeholder": "​",
      "style": "IPY_MODEL_78ff1615fa4b4d8ab51c4683a43f4182",
      "value": " 1.00k/1.00k [00:00&lt;00:00, 31.8kB/s]"
     }
    },
    "c20354dbe20844c28fe1455c765db2f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c677b05ae99841c6879009aa20f2e5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cab874fdbbc0434f876661f578f3f855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_daf3185604a54f268a5b5f3a456c0f01",
      "placeholder": "​",
      "style": "IPY_MODEL_7a12232f55c5447fb0284ca001cc0023",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "d50332475bbe429f9ef61ba26609cdc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daf3185604a54f268a5b5f3a456c0f01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f541cd5c2c704129855a39d1c81a48b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa47d352268a4be388b820f9a4a6c5a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
