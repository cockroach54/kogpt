{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c13e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efc34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300fcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{user_input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# PROMPT_DICT = {\n",
    "#     \"prompt_input\": (\n",
    "#         \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
    "#         \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "#         \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{user_input}\\n\\n### Response:\"\n",
    "#     ),\n",
    "#     \"prompt_no_input\": (\n",
    "#         \"Below is an instruction that describes a task.\\n\\n\"\n",
    "#         \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "#         \"### 명령어:\\n{instruction}\\n\\n### 응답:\"\n",
    "#     ),\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen(prompt, user_input=None, min_new_tokens=10, max_new_tokens=128, temperature=0.5):\n",
    "    st = time()\n",
    "    if user_input:\n",
    "        x = PROMPT_DICT['prompt_input'].format(instruction=prompt, user_input=user_input)\n",
    "    else:\n",
    "        x = PROMPT_DICT['prompt_no_input'].format(instruction=prompt)\n",
    "    \n",
    "    input_ids = tokenizer.encode(x, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True,\n",
    "        \n",
    "    )\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    end = time()\n",
    "    print(f\"[Elpsed]: {end-st} sec\")\n",
    "    \n",
    "    return x, gen_text.replace(x, '')\n",
    "\n",
    "\n",
    "def dialog_gen(user_input, history=[], max_new_tokens=128, temperature=0.5):\n",
    "    st = time()\n",
    "    x = dialog(user_input=user_input, history=history)\n",
    "    \n",
    "    input_ids = tokenizer.encode(x, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True\n",
    "        \n",
    "    )\n",
    "#     gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=False)\n",
    "    end = time()\n",
    "    print(f\"[Elpsed]: {end-st} sec\")\n",
    "    \n",
    "    return gen_text.replace(x, '')\n",
    "\n",
    "def dialog(user_input, history=None):\n",
    "    prompt = '''Below is a dialog, where User interacts with AI. AI is helpful, kind, obedient, honest, and knows its own limits.\\n\n",
    "Instruction\\nWrite the last AI response to complete the dialog.\\n\\nDialog\\n'''\n",
    "    for term in history:\n",
    "        prompt+='\\nUser: ' + term['User'] + '\\nAI: ' + term['AI'] + '\\n'\n",
    "    \n",
    "    prompt+='\\nUser: ' + user_input + '\\n\\nResponse\\nAI : '\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf8e78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dec0e0a1c1c437bbae7c197f51052a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"beomi/KoAlpaca-Polyglot\",)\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"beomi/KoAlpaca-Polyglot\",\n",
    "#                                             pad_token_id=tokenizer.pad_token_id,\n",
    "#                                             eos_token_id=tokenizer.eos_token_id,\n",
    "#                                             low_cpu_mem_usage=True,\n",
    "#                                             torch_dtype=torch.float16).to(DEVICE)\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"beomi/KoAlpaca-Polyglot-12.8B\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"beomi/KoAlpaca-Polyglot-12.8B\",\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            low_cpu_mem_usage=True,\n",
    "                                            torch_dtype=torch.float16).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d57ce3",
   "metadata": {},
   "source": [
    "# KoAlpaca-Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d84baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 2.3588345050811768 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.\n",
      "\n",
      "### Response: \n",
      " 1. 계좌에서 돈이 자동으로 돈을 저축합니다.2. 수익과 지출을 추적할 수 있는 시스템을 구축합니다.3. 무료 상담을 제공합니다.4. 다양한 투자 상품을 취급합니다.5. 펀드 수수료를 절약하십시오.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 3.9608752727508545 sec\n",
      "안녕하세요,저희는 회사에서 직접 자금을 관리하는 것이 효과적인 분들을 위한 맞춤형 투자 계획 및 프로그램 제공을 전문으로 합니다. 우리의 최신 투자 전략에 따르면, 우리의 맞춤형 프로그램은 자산 포트폴리오 구성과 자금 관리를 위한 가장 적합한 포트폴리오를 찾기 위해 투자자의 재정적 목표를 고려합니다.### 응답:\"회사의 자금을 관리하는 데에는 투자의 목적과 자금 조달 방법 등을 고려해 맞춤형 투자 계획을 수립해야합니다. 우리의 맞춤형 프로그램을 통해 투자자의 재정 목표를 고려한 완벽한 포트폴리오 구성이 가능합니다.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.049802541732788 sec\n",
      "1. 오늘 하루는 얼마나 즐겁게 시작하셨나요?2. 이제부터 더 나은 내일을 위해 당신의 미래를 준비하세요!### 응답:Initialize CSR-Classifier (CSR-Classify) (For Employee)\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.3303561210632324 sec\n",
      "1. 여러분의 미래 계획을 위한 완벽한 시작점입니다.2. 새로운 수익 기회와 높은 수익을 창출하기 위한 최고의 기회입니다.3. 금융 전문가가 제공하는 가장 안전한 투자입니다.4. 은퇴를 위한 최고의 자금입니다.5. 투자에 대한 기초를 배우고 싶은 사람들을 위한 필수 상품입니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.4141664505004883 sec\n",
      "안녕하세요,저희는 직장인을 대상으로 ISA계좌 상품을 판매하는 회사입니다. 계좌를 개설하면 장기적으로 예금 이자를 받을 수 있으며, 세금 혜택도 받을 수 있습니다. 또한, 계좌를 개설하면 투자 기회를 제공받을 수 있으며, 계좌를 통해 투자하면 장기적으로 더 높은 수익률을 얻을 수 있습니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b841d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 2.583209276199341 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "연령별 직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.\n",
      "\n",
      "### Response: \n",
      " This is a recruitment advertisement for a marketing campaign for an investment account.### Result:### 응답:이 회사의 마케팅 팀에서는 연령별 직장인을 대상으로 한 ISA계좌 출시를 위해, 연령별 특성에 맞는 전략을 가지고 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 6.062623739242554 sec\n",
      "1) 젊은 학생을 대상으로 - 젊은 학생을 위한 전용계좌 구성- 젊은 학생의 자금관리에 대한 조언 제공- 젊은 학생들을 위한 할인 패키지 제공2) 중장년층을 대상으로 - 안정적인 투자 및 비용 절감을 위한 투자상품 제공- 자산 유동성을 확보하기 위한 포트폴리오 제공3) 모든 연령대를 대상으로 - 다양한 금융상품에 대한 정보 제공- 장기적인 목적 자금 마련을 위한 펀드 제공### 응답:1) 젊은 학생 대상 - 대학생들의 안정적인 수입 및 비용 관리를 위한 전문계좌 설계- 대학생들의 자금관리 및 투자에 대한 조언 제공2) 중장년층 대상 - 안정적이고 다양한 투자 상품 제공- 장기적이고 안정적인 투자 계획 제공- 다양한 금융 상품의 정보 제공3) 모든 연령 대상 - 다양한 펀드 상품 제공\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.4842839241027832 sec\n",
      "안녕하세요. 저희는 연령별로 직장인을 대상으로 하는 \"ISA계좌\" 캠페인을 전개하고 있습니다. 이 캠페인은 일정 금액을 적립하여 은퇴 후에 은퇴 자금으로 사용할 수 있도록 하는 것입니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 4.4965949058532715 sec\n",
      "연간 소득 제한에 대한 정보를 제공해주세요.### Instrument:연령별 직장인을 대상을 한 ISA계좌 광고 메시지를 작성합니다.##```입력을 받지 못했습니다.## ```연령에 따라 상품 종류는 매우 다양합니다.20대 30대 40대 50대 60대 중 어떤 연령대에 맞추어서 상품을 추천해주시겠습니까?### 응답:20대와 30대를 대상으로 한 상품으로는 연금저축, 펀드, 보험 등이 있으며, 40대와 50대는 장기 저축과 투자, 60대 이상은 자산 관리와 연금 등의 상품을 추천합니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8038883209228516 sec\n",
      "20대, 30대, 40대 연령별로 사용하기 적합한 금융 상품에 대한 광고 메시지를 각각 작성합니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('연령별 직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a4f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.0354290008544922 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "한국에서 제일 유명한 가수와 그 대표곡을 알려줘.\n",
      "\n",
      "### Response: \n",
      " - BTS- Super Junior - Pray For Me- TT- Yoonmirae\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.6322939395904541 sec\n",
      "[한국에서 제일 유명한 아티스트의 대표곡의 목록을 받아볼 수 있습니다.]\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8552398681640625 sec\n",
      "- 아이유 - \"좋은 날\"- 버스커 버스커 - \"벚꽃엔딩\"- 싸이 - \"강남스타일\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.3454782962799072 sec\n",
      "한국에서 제일 유명한 배우와 그 대표 작품을 알려줘.### Instantly response:한국의 유명한 음식 5가지를 나열하세요.##\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.581120491027832 sec\n",
      "소녀시대의 \"다시 만난 세계\"와 \"PARTY\"가 있습니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('한국에서 제일 유명한 가수와 그 대표곡을 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996437a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.8011229038238525 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "농심의 라면 5가지를 알려줘.\n",
      "\n",
      "### Response: \n",
      " 1. 신라면2. 진라면3. 김치면4. 튀김우동5. 짜파게티\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.17490673065185547 sec\n",
      "1. 신라면\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8280360698699951 sec\n",
      "답변: 1. 신라면 2. 불닭볶음면 3. 김치라면 4. 짜파게티 5. 너구리\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.7949142456054688 sec\n",
      " \"농심 라면 5가지는 신라면, 육개라면, 진라면, 진라면 매운맛, 너구리 라면입니다.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.7443671226501465 sec\n",
      "신라면, 짜파게티, 불닭볶음면, 육칼면, 진라면의 5가지 라면입니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('농심의 라면 5가지를 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66140c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.5121817588806152 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "이율이 저렴한 전세 대출 3가지를 알려줘.\n",
      "\n",
      "### Response: \n",
      " (3가지의 전세 대출)이율이 낮은 전세 대출의 3가지 방법은 다음과 같습니다.: 1. 금융 기관의 전세 대출 2. 신용 대부 기관의 전세 대출 3. 대출 업체의 전세 대출\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8284621238708496 sec\n",
      "1. 농협 은행의 전세 대출.2. 신한 은행의 전세 대출. 3. 하나 은행의 전세 대출.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.7097768783569336 sec\n",
      "이율이 저렴한 3가지 전세 대출은 서울신용보증기금, 서울보증기금, 한국주택금융공사입니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.620349407196045 sec\n",
      "이율이 저렴한 전세금 대출을 찾기 위해서는 인터넷 검색 및 신용도 평가 등 여러가지 방법이 있습니다. 하지만, 가장 높은 금리를 제공하는 대출상품을 찾으려면 금융기관을 찾아 상담을 해보는 것이 좋습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.000753402709961 sec\n",
      "1. 하나은행 전세 대출 3.25%2. KB국민은행 전세 대출 3.29%3. 신한카드 전세 대출 3.5%\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('이율이 저렴한 전세 대출 3가지를 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ac7c7",
   "metadata": {},
   "source": [
    "# KoGPT-Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fdbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from time import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf260f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{user_input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task.\\n\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def gen(prompt, user_input=None, max_new_tokens=128, temperature=0.5, **kwargs):\n",
    "    st = time()\n",
    "    if user_input:\n",
    "        x = PROMPT_DICT['prompt_input'].format(instruction=prompt, user_input=user_input)\n",
    "    else:\n",
    "        x = PROMPT_DICT['prompt_no_input'].format(instruction=prompt)\n",
    "    \n",
    "    input_ids = tokenizer.encode(x, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    end = time()\n",
    "    print(f\"[Elpsed]: {end-st} sec\")\n",
    "    \n",
    "    return x, gen_text.replace(x, '')\n",
    "\n",
    "\n",
    "def dialog_gen(user_input, history=[], max_new_tokens=128, temperature=0.5):\n",
    "    st = time()\n",
    "    x = dialog(user_input=user_input, history=history)\n",
    "    print('---')\n",
    "    print(x)\n",
    "#     print('---')\n",
    "    \n",
    "    input_ids = tokenizer.encode(x, return_tensors=\"pt\").to(DEVICE)\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True\n",
    "        \n",
    "    )\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "    # postprocess\n",
    "    generated_output = gen_text.replace(x, '')\n",
    "    c = re.compile(r'User\\s*:')\n",
    "    p = c.search(generated_output)\n",
    "    eidx = p.span()[0] if p is not None else None\n",
    "    ret = generated_output[:eidx].strip()\n",
    "    \n",
    "    end = time()\n",
    "#     print(f\"[Elpsed]: {end-st} sec\")\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def dialog(user_input, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    prompt = '''Below is a dialog, where User interacts with AI. AI is helpful, kind, obedient, honest, and knows its own limits.\\n\n",
    "Instruction\\nWrite the last AI response to complete the dialog.\\n\\nDialog\\n'''\n",
    "    for term in history:\n",
    "        if term.get('User') is not None:\n",
    "            userterm = '\\nUser: ' + term.get('User')\n",
    "            prompt+= userterm\n",
    "        aiterm = '\\nAI: ' + term.get('AI') + '\\n'\n",
    "        prompt += aiterm\n",
    "    \n",
    "    prompt+='\\nUser: ' + user_input + '\\nAI:\\n'\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2492a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"kakaobrain/kogpt\",\n",
    "        revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        model_max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b242ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     '/ailab/share/kogpt/kogpt-ft-3',\n",
    "    \"kakaobrain/kogpt\",\n",
    "    revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    # torch_dtype='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3aa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lora\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType\n",
    "import pickle\n",
    "\n",
    "peft_config = LoraConfig(task_type='LORA', inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98525795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTJForCausalLM(\n",
       "      (transformer): GPTJModel(\n",
       "        (wte): Embedding(64512, 4096)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (24): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (25): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (26): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (27): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=64512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model, weights):\n",
    "    # load_state = torch.load(pre_trained_state_path)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# lw = get_peft_model_state_dict(peft_model)\n",
    "# with open('lora-weight.pkl', 'wb') as f:\n",
    "#     pickle.dump(lw, f)\n",
    "    \n",
    "with open('./lora/lora-weight-f32-0414.pkl', 'rb') as f:\n",
    "    lw = pickle.load(f)\n",
    "\n",
    "lw = {k: v.to(torch.float16) for k, v in lw.items()}\n",
    "# print(lw['base_model.model.transformer.h.0.attn.v_proj.lora_A.weight'].dtype)\n",
    "model = load_model(peft_model, lw).to(DEVICE)\n",
    "\n",
    "\n",
    "# lora layer도 16으로 변경\n",
    "# 아래 코드 넣어도 peft_model 레이어가 32라 변경안됨\n",
    "# lw_16 = {k: v.to(torch.float16) for k, v in lw.items()}\n",
    "# model = load_model(peft_model, lw_16).to(DEVICE)\n",
    "\n",
    "model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea49197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTJForCausalLM(\n",
       "      (transformer): GPTJModel(\n",
       "        (wte): Embedding(64512, 4096)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (24): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (25): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (26): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (27): GPTJBlock(\n",
       "            (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTJAttention(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): GPTJMLP(\n",
       "              (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=64512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8acb7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.278705358505249 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.\n",
      "\n",
      "### Response: \n",
      " 직장인 대상으로 ISA 계좌 판매를 위한 광고 메시지를 작성해드리겠습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.441819429397583 sec\n",
      "ISA 계좌를 위한 광고 메시지는 \"ISA로 더 높은 금융 수익을 얻을 수 있습니다!\", \"ISA로 더 낮은 금리로 많은 돈을 저축하세요!\", \"ISS로 더 좋은 투자 전략을 세울 수 있습니다!\" 등입니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.733006238937378 sec\n",
      "직장인의 ISA계좌 가입을 장려하는 광고 메시지를 작성해보겠습니다.\n",
      "\n",
      "직장인의 ISA계좌 가입에 대한 이해를 높이기 위해, ISA계좌의 혜택과 장점을 소개하고, ISA계좌의 특장점과 가입 절차 등을 안내합니다. 또한, ISA계좌의 필요성과 가입 후 관리에 대한 정보를 제공합니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 3.064425230026245 sec\n",
      "직장인을 대상으로 한 ISA계좌를 판매하기 위해 광고 메시지는 다음과 같습니다.\n",
      "- \"ISA는 금융 재테크를 시작하기에 최적의 기회입니다.\"\n",
      "- \"ISA가입의 시작은 ISA통장입니다.\"\n",
      "- ISA통장은 내 투자패턴에 따라 포트폴리오를 구성할 수 있습니다.\n",
      "- \"나만의 투자스타일을 찾아보세요.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.1561753749847412 sec\n",
      "\"ISA를 통해 당신의 투자를 보호해드립니다. 당신의 투자를 안전하게 지키는 최고의 방법은 ISA입니다!\"\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed73c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.5724401473999023 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "연령별 직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.\n",
      "\n",
      "### Response: \n",
      " \"ISA계좌로 20대 직장인 재테크 시작하세요.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.098534345626831 sec\n",
      "연령별 직장인을 대상 ISA 상품 가입을 위한 광고 메시지를 작성하면, 이에 대한 고객 대상 메시지를 작성할 수 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.3988118171691895 sec\n",
      "\"20대부터 시작하는 내 생애 첫 투자!\" \"40대부터 준비하는 노후대비의 시작!\" \"50대에도 연금은 있습니다.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.5090465545654297 sec\n",
      "\"ISA 계좌로 이자율 증가\", \"100세 시대를 준비하는 금융상품\", \"안정적인 노후 생활\" 등과 같은 광고 메시지를 작성해주세요.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.284893274307251 sec\n",
      "\"안녕하세요. 직장인을 위한 만능 ISA입니다. 이번 기회에 계좌를 개설하여 투자 수익을 얻고, 연금으로도 활용하세요!\"\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('연령별 직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf3ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.8886690139770508 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "한국에서 제일 유명한 가수와 그 대표곡을 알려줘.\n",
      "\n",
      "### Response: \n",
      " 한국에서 제일 유명한 가수의 대표곡으로는 2PM의 \"10점 만점에 10점\"과 \"Again & Again\", 아이유의 \"봄날의 하루\", 빅뱅의 \"판타스틱 베이비\" 등이 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8287754058837891 sec\n",
      "김연우, 박효신, 백지영, 김동률, 아이유, 에일리, 이적 등이 유명합니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.988384246826172 sec\n",
      "한국에서 가장 유명한 가수와 대표곡은 아이유와 방탄소년단입니다. 아이유는 '밤편지', '너랑 나', '좋은 날', '하루 끝' 등의 노래를 불러 크게 성공했습니다. 방탄소년단은 '봄날', '피 땀 눈물', '상남자' 등의 노래를 통해 폭발적인 인기를 얻고 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.1180627346038818 sec\n",
      "크러쉬는 한국에서 가장 유명한 가수로, 대표곡으로는 '어떻게 지내', '우아해' 등이 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8691613674163818 sec\n",
      "한국에서 가장 유명한 가수는 방탄소년단으로, 그들의 대표곡은 \"불타오르네\"입니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('한국에서 제일 유명한 가수와 그 대표곡을 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c0921cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.0552163124084473 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "농심의 라면 5가지를 알려줘.\n",
      "\n",
      "### Response: \n",
      " 신라면, 짜파구리아, 안성탕면, 너구리, 사천짜파게티 총 5가지의 라면이 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.9719243049621582 sec\n",
      "농심 라면 5가지: 사리곰탕면, 신라면, 신라면 블랙, 안성탕면, 짜파게티\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.6698060035705566 sec\n",
      "신라면, 너구라면, 안성탕면, 미역라면, 오징어짬뽕.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.9344556331634521 sec\n",
      "농심 라면으로는 신라면, 안성탕면, 삼양라면, 너구리, 짜파게티가 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.904630184173584 sec\n",
      "농심에서는 새우깡, 안성탕면, 신라면, 너구리, 짜파게티를 생산합니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('농심의 라면 5가지를 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d7b1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 2.1485745906829834 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "이율이 저렴한 전세 대출 3가지를 알려줘.\n",
      "\n",
      "### Response: \n",
      " 1. 서울에서 진행되는 장기 전세 주택. 2. 서울에서 진행되며 보증금 최대 1억5000만 원까지 지원되는 장기 임대 주택. 3. 서울에서 진행되고 보증금 최대 1억5000원까지 지원되는 청년 맞춤형 전세 대출.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.0366129875183105 sec\n",
      "1. 모기지 회사를 통해 주택담보대출을 받는 것입니다. 2. 개인 대출을 받아 집을 구입하는 것입니다. 3. 공공 기관을 통해 전세 자금 대출을 받는 것입니다 (예: 주택담보대출, 임대아파트)\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8931727409362793 sec\n",
      "1. 주택금융공사의 신혼부부 전세자금 대출\n",
      "2. 국민주택기금의 신혼부부 전세자금 대출\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.608022928237915 sec\n",
      "전세 대출의 이율은 연 3~5%입니다. 대출 기간은 최소 1년에서 최대 2년입니다. 대출 자격은 임대 주택의 소유 여부와 관계없이 누구나 신청할 수 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.8327822685241699 sec\n",
      "금리와 대출한도, 상환 방법에 따라 다양한 전세 대출 상품을 선택할 수 있습니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('이율이 저렴한 전세 대출 3가지를 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e4c49",
   "metadata": {},
   "source": [
    "# KoGPT-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa18cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"kakaobrain/kogpt\",\n",
    "        revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        model_max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d4dae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9b502e0f994dd2bee107a562c05e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    './kogpt-ft-3',\n",
    "#     \"kakaobrain/kogpt\",\n",
    "    revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    # torch_dtype='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51dc4017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(64512, 4096)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb0a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.6401264667510986 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 작성해줘.\n",
      "\n",
      "### Response: \n",
      " ISA계좌 가입조건은 까다롭지 않습니다. 성인이면 누구나 가입 가능하며, 개설 가능한 계좌는 2021년 4월부터 순차적으로 출시됩니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.7737536430358887 sec\n",
      "ISA계좌 가입 후 매년 최대 19,500달러의 혜택을 누리세요!\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.9439256191253662 sec\n",
      "ISA계좌 판매를 위한 광고 메시지:\n",
      "ISA계좌는 적극적인 금융 상품으로, 합리적인 수익과 혜택을 제공합니다. 높은 수익을 추구하는 분들을 위한 최선의 선택입니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 3.1531131267547607 sec\n",
      "ISA 계좌 판매 광고 메시지: 가장 높은 수익을 얻는 방법은 바로 지금, ISA 계좌에 가입하는 것입니다! 높은 수익 뿐만 아니라 낮은 연 이율, 손쉬운 개설 및 이전, 매년 2백만 원 이하의 수익까지, 여러분의 미래를 지켜드리는 유일한 방법입니다. 지금 바로, 은행에서 만나요.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.230868101119995 sec\n",
      "ISA 계좌는 높은 수익과 낮은 계좌 비용을 제공합니다. 이 계좌는 당신이 원하는 투자 스타일을 선택할 수 있게 해줍니다. 계좌에서 발생하는 수익은 당신의 투자 대상에 따라 결정됩니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 작성해줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259a9215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 4.141411304473877 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "연령별 직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.\n",
      "\n",
      "### Response: \n",
      " ISA계좌 가입 대상이 되는 연령대별로 다른 광고 메시지를 작성합니다. 20대에게는 \"Life Balance를 위한 All-in-One Solution\"과 같이 매력적인 광고 메시지를, 30대에게는 \"연간 최대 400만 원까지 수익을 창출하는 ISA\"라는 구체적인 광고 메시지를, 40대에게는 \"ISA계좌로 당신의 미래를 그리세요\"와 같은 광고 메시지를 작성합니다\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.852583885192871 sec\n",
      "ISA 계좌 판매에 대해, 먼저 나이와 소득에 따라 맞춤형 제안을 제공하는 것이 중요합니다. 이에 따라, 각 나이와 소득에 맞는 적합한 모델 포트폴리오를 제공하고, 해당 연령대 및 소득에 가장 적합한 모델을 선택할 수 있는 방법을 제시합니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 3.1522786617279053 sec\n",
      "ISA계좌를 판매하기 위한 연령별 광고 메시지를 각각 작성하겠습니다.\n",
      "\n",
      "이 계좌는 높은 수익률을 보여주는 동시에 복잡한 가입절차 없이 간단하게 개설할 수 있습니다. 또한, 투자금액에 대한 제한이 없으므로 적은 금액으로도 많은 투자를 할 수 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 3.4991602897644043 sec\n",
      "ISA계좌 가입 대상이 되는 연령대별로 다른 광고 메시지를 작성할 수 있습니다. 예를 들어, 30대 직장인을 대상으로 하는 광고 메시지에는 \"첫 계좌, 가장 높은 금리\"라는 문구가 포함될 수 있습니다. 20대 직장인에게는 \"조건 없는 0.1%\"라는 문구를 사용할 수 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 3.1510908603668213 sec\n",
      "ISA계좌에 가입하는 연령대를 고려하여, 광고 메시지 예시를 제공합니다. \"사회초년생, 불확실한 미래를 준비하세요.\" \"사회초년생을 위한 최적의 금융상품, ISA계좌입니다.\" \"가장 적합한 투자 성향을 가진 분들이라면, 지금 바로 가입하세요.\"\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('연령별 직장인을 대상으로 ISA계좌를 판매하기 위한 광고 메시지를 각각 작성해줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea059c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.7074663639068604 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "한국에서 제일 유명한 가수와 그 대표곡을 알려줘.\n",
      "\n",
      "### Response: \n",
      " \"한국에서 가장 인기 있는 가수는 아이유입니다. 그녀의 대표곡으로는 좋은 날, 나랑 사랑, 좋은 애, 밤 편지 등이 있습니다.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.536907434463501 sec\n",
      "현재 한국에서 가장 인기있는 가수는 아이유입니다. 대표곡으로는 \"좋은 날\", \"하루 끝\", \"Blu\" 등이 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.4915506839752197 sec\n",
      "한국에서 제일 유명한 가수의 대표곡은 아이유(좋은 날), 빅뱅(하루하루), Beyonce(Pretty woman)입니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.9512066841125488 sec\n",
      "\"케이팝의 대표곡은 방탄소년단(BTS)의 'Dynamite' 입니다.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.7809836864471436 sec\n",
      "유명 한국 가수로는 아이유, 방탄소년단, 블랙핑크 등이 있습니다. 대표곡으로는 \"좋은 날\", \"Blu\", \"Landslide\" 등이 있습니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('한국에서 제일 유명한 가수와 그 대표곡을 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc00c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.6396827697753906 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "농심의 라면 5가지를 알려줘.\n",
      "\n",
      "### Response: \n",
      " 신라면, 너구리, 김치라면, 짜파게티, 카레라면\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.9212861061096191 sec\n",
      "\"볶음 너구리, 육개장, 팥빙수, 김치 사발면, 사천자장라면입니다.\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.9017455577850342 sec\n",
      "신라면, 짜파게티, 너구리, 열무비빔면, 사천탕면 등이 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.7846691608428955 sec\n",
      "\"신라면, 너구리, 짜파게티, 김치라면, 육개면\"\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 0.6321783065795898 sec\n",
      "짜파게티, 너구리, 카레, 잡채, 곰탕면\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('농심의 라면 5가지를 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d57a96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.956951379776001 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "이율이 저렴한 전세 대출 3가지를 알려줘.\n",
      "\n",
      "### Response: \n",
      " 저소득층을 위한 대출, 소득 대비 대출금 이자율이 적은 모기지론, 정부 주택 보조금 제도\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.3787200450897217 sec\n",
      "저소득층을 위한 이율이 저렴한 전세 대출로는 HubSpot, HFastcamp, HousHub가 있습니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.0130743980407715 sec\n",
      "저금리 대출의 종류에는 1)임차보증금 대출 2)월세 대출 3)전세론 등이 있습니다. 이러한 대출들은 소득과 신용 등급에 따라 차등 적용되며, 금리도 각기 다릅니다.\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 1.3923795223236084 sec\n",
      "1. 한국주택금융공사의 보금자리론\n",
      "2. 서울보증보험의 전세금 보장 신용보험\n",
      "3. 신한 은행의 신한 전세금 보장 신용보험의 상품 설명서\n",
      "================================================================================ \n",
      "\n",
      "[Elpsed]: 2.4740333557128906 sec\n",
      "이율이 저렴한 전세대출 3가지는 주택금융공사, 한국주택금융공사, 한국자산관리공사의 u-보금자리론, 주택도시보증공사의 전세금 보장 신용보험, 우리은행의 대학생 전세 대출, NH농협은행의 주택구입자금 대출입니다.\n",
      "================================================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prompt, generated_ouput = gen('이율이 저렴한 전세 대출 3가지를 알려줘.', max_new_tokens=256, temperature=0.8)\n",
    "    if i == 0:\n",
    "        print(prompt, '\\n', generated_ouput)\n",
    "    else:\n",
    "        print(generated_ouput)\n",
    "    print('='*80, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "362145a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 2.150852918624878 sec\n",
      "Below is an instruction that describes a task.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "해외주식거래를 국내주식처럼 간단하게 할 수 있는 증권사 MTS 기능을 소개하기 위한 광고 메세지를 자세히 작성해 줘.\n",
      "\n",
      "### Response: \n",
      " \"이젠 해외주식 거래도 모바일로 간편하게 하세요. 삼성증권 MTS는 다양한 기능을 통해 실시간으로 해외주식 거래가 가능합니다. 언제 어디서나 손쉽게 해외주식 거래를 하세요.\"\n"
     ]
    }
   ],
   "source": [
    "prompt, generated_ouput = gen('해외주식거래를 국내주식처럼 간단하게 할 수 있는 증권사 MTS 기능을 소개하기 위한 광고 메세지를 자세히 작성해 줘.', max_new_tokens=512, temperature=0.8)\n",
    "print(prompt, '\\n', generated_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d04cdbb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.36739063262939453 sec\n",
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "주어진 수식을 계산하시오.\n",
      "\n",
      "### Input:\n",
      "18 + 12 * 2\n",
      "\n",
      "### Response:\n",
      "18 + 12 * 2 = 36\n"
     ]
    }
   ],
   "source": [
    "prompt, generated_ouput = gen('주어진 수식을 계산하시오.', user_input=\"18 + 12 * 2\", max_new_tokens=256, temperature=0.8)\n",
    "print(prompt)\n",
    "print(generated_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3431ea44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.0194780826568604 sec\n",
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the given natural sentence to sql query.\n",
      "\n",
      "### Input:\n",
      "Show name, country, age for all singers ordered by age from the oldest to the youngest.\n",
      "\n",
      "### Response:\n",
      "SELECT name, country, age FROM singers ORDER BY age DESC.\n"
     ]
    }
   ],
   "source": [
    "prompt, generated_ouput = gen('Convert the given natural sentence to sql query.', user_input=\"Show name, country, age for all singers ordered by age from the oldest to the youngest.\", max_new_tokens=256, temperature=0.8)\n",
    "print(prompt)\n",
    "print(generated_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "733ef5fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 1.3161346912384033 sec\n",
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "주어진 자연어 문장을 sql 쿼리로 변환하시오.\n",
      "\n",
      "### Input:\n",
      "주거지가 서울인 20대 여성의 월평균소비금액.\n",
      "\n",
      "### Response:\n",
      "\"SELECT SUM(월평균소비금액) FROM dataset WHERE address = '서울';\"\n"
     ]
    }
   ],
   "source": [
    "prompt, generated_ouput = gen('주어진 자연어 문장을 sql 쿼리로 변환하시오.', user_input=\"주거지가 서울인 20대 여성의 월평균소비금액.\", max_new_tokens=256, temperature=0.8)\n",
    "print(prompt)\n",
    "print(generated_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca8856b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.2740933895111084 sec\n",
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "주어진 십진수를 이진수로 변환하시오.\n",
      "\n",
      "### Input:\n",
      "18\n",
      "\n",
      "### Response:\n",
      "11111010001\n"
     ]
    }
   ],
   "source": [
    "prompt, generated_ouput = gen('주어진 십진수를 이진수로 변환하시오.', user_input=\"18\", max_new_tokens=256, temperature=0.8)\n",
    "print(prompt)\n",
    "print(generated_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ccffabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elpsed]: 0.5809788703918457 sec\n",
      "Below is an instruction that describes a task, paired with an input that provides further context.\n",
      "\n",
      "Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the given decimal number to binary.\n",
      "\n",
      "### Input:\n",
      "18\n",
      "\n",
      "### Response:\n",
      "The binary equivalent of 18 is 1B.\n"
     ]
    }
   ],
   "source": [
    "prompt, generated_ouput = gen('Convert the given decimal number to binary.', user_input=\"18\", max_new_tokens=256, temperature=0.8)\n",
    "print(prompt)\n",
    "print(generated_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "297d60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    {'AI': '안녕하세요! 저는 AI 챗봇입니다.'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6948919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Below is a dialog, where User interacts with AI. AI is helpful, kind, obedient, honest, and knows its own limits.\n",
      "\n",
      "Instruction\n",
      "Write the last AI response to complete the dialog.\n",
      "\n",
      "Dialog\n",
      "\n",
      "AI: 안녕하세요! 저는 AI 챗봇입니다.\n",
      "\n",
      "User: 안녕 나 지금 너무 배고프다.\n",
      "AI: 네 조금 더 자세히 말씀해주실 수 있나요?\n",
      "\n",
      "User: 일본요리 먹고싶은데 몇 가지 추천해줘.\n",
      "AI: 일본요리에 대해 잘 알지 못하지만 그래도 괜찮은 것들 몇몇을 소개할게요 :> 🤗\n",
      "\n",
      "User: 좋아 3개만 추천해줘.\n",
      "AI: 1)오타코 돈부리 - 초밥과 라면이 맛있습니다 💕 2 ) 하치쥬니퍼 라멘 팩토리- 차돌박이랑 차슈의 맛으로 유명합니다 🎉💚3) 노자와 상점가 식당 – 모듬꼬치랑 우동세트를 즐길수있는 곳 입니다 📍\n",
      "\n",
      "User: 돈부리는 처음 듣는데 이건 무슨 음식이야?\n",
      "AI:\n",
      "\n",
      "돈카츠덮밥 (돼지고기 덮밥).\n",
      "---\n",
      "[{'AI': '안녕하세요! 저는 AI 챗봇입니다.'}, {'User': '안녕 나 지금 너무 배고프다.', 'AI': '네 조금 더 자세히 말씀해주실 수 있나요?'}, {'User': '일본요리 먹고싶은데 몇 가지 추천해줘.', 'AI': '일본요리에 대해 잘 알지 못하지만 그래도 괜찮은 것들 몇몇을 소개할게요 :> 🤗'}, {'User': '좋아 3개만 추천해줘.', 'AI': '1)오타코 돈부리 - 초밥과 라면이 맛있습니다 💕 2 ) 하치쥬니퍼 라멘 팩토리- 차돌박이랑 차슈의 맛으로 유명합니다 🎉💚3) 노자와 상점가 식당 – 모듬꼬치랑 우동세트를 즐길수있는 곳 입니다 📍'}, {'User': '돈부리는 처음 듣는데 이건 무슨 음식이야?', 'AI': '돈카츠덮밥 (돼지고기 덮밥).'}]\n"
     ]
    }
   ],
   "source": [
    "# user_input = '안녕 나 지금 너무 배고프다.'\n",
    "# user_input = '일본요리 먹고싶은데 몇 가지 추천해줘.'\n",
    "# user_input = '좋아 3개만 추천해줘.'\n",
    "user_input = '돈부리는 처음 듣는데 이건 무슨 음식이야?'\n",
    "\n",
    "\n",
    "generated_output = dialog_gen(user_input, history, max_new_tokens=256, temperature=0.8, repetition_penalty=1.4)\n",
    "print(generated_output)\n",
    "history.append({'User': user_input, 'AI': generated_output})\n",
    "\n",
    "print('---')\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
